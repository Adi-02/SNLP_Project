{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if GPU has free memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def get_gpu_memory():\n",
    "    result = subprocess.run(\n",
    "        ['nvidia-smi', '--query-gpu=memory.free,memory.total', '--format=csv,nounits,noheader'],\n",
    "        stdout=subprocess.PIPE, text=True\n",
    "    )\n",
    "    lines = result.stdout.strip().split('\\n')\n",
    "    for i, line in enumerate(lines):\n",
    "        free, total = map(int, line.split(','))\n",
    "        print(f\"GPU {i}: {free / 1024: .4} GiB free / {total / 1024: .4} GiB total\")\n",
    "\n",
    "get_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_DIR = Path.cwd()\n",
    "HAYSTACK_DIR = PROJECT_DIR / \"haystack\"\n",
    "RELEVANT_DIR = HAYSTACK_DIR / \"relevant\"\n",
    "IRRELAVANT_DIR = HAYSTACK_DIR / \"irrelevant\"\n",
    "\n",
    "sys.path.append(str(PROJECT_DIR))\n",
    "\n",
    "# Set Hugging Face cache path. Use Absolute path.\n",
    "os.environ['HF_HOME'] = '/cs/student/projects1/2021/aagarwal/SNLP_Project/.cache/hf_with_quota'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from argparse import Namespace\n",
    "\n",
    "def get_args(id, context_type, data):\n",
    "    \n",
    "    item = next(item for item in data if item[\"id\"] == id)\n",
    "\n",
    "    if context_type == \"relevant\":\n",
    "        haystack_file = RELEVANT_DIR / item[\"context_relevant\"]\n",
    "    elif context_type == \"irrelevant\":\n",
    "        haystack_file = IRRELAVANT_DIR / item[\"context_irrelevant\"]\n",
    "\n",
    "    args = Namespace(\n",
    "        model_name = \"yaofu/llama-2-7b-80k\",\n",
    "        model_name_suffix = f\"{context_type}_id_{item['id']}\",\n",
    "        model_provider = \"LLaMA\",\n",
    "\n",
    "        context_lengths_min = 0,\n",
    "        context_lengths_max = 5000,\n",
    "        context_lengths_num_intervals = 10,\n",
    "\n",
    "        document_depth_percent_intervals = 10,\n",
    "\n",
    "        needle = item[\"needle\"],\n",
    "        real_needle = item[\"real_needle\"],\n",
    "        retrieval_question = item[\"question\"],\n",
    "        haystack_file = haystack_file,\n",
    "    )\n",
    "\n",
    "    return args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from retrieval_head_detection import LLMNeedleHaystackTester as Tester\n",
    "\n",
    "with open(HAYSTACK_DIR / \"needles.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "args = get_args(id=44, context_type=\"relevant\", data=data)\n",
    "tester = Tester(**vars(args))\n",
    "\n",
    "tester.start_test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
