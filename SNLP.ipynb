{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if GPU has free memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0:  23.46 GiB free /  23.99 GiB total\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "def get_gpu_memory():\n",
    "    result = subprocess.run(\n",
    "        ['nvidia-smi', '--query-gpu=memory.free,memory.total', '--format=csv,nounits,noheader'],\n",
    "        stdout=subprocess.PIPE, text=True\n",
    "    )\n",
    "    lines = result.stdout.strip().split('\\n')\n",
    "    for i, line in enumerate(lines):\n",
    "        free, total = map(int, line.split(','))\n",
    "        print(f\"GPU {i}: {free / 1024: .4} GiB free / {total / 1024: .4} GiB total\")\n",
    "\n",
    "get_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_DIR = Path.cwd()\n",
    "HAYSTACK_DIR = PROJECT_DIR / \"haystack\"\n",
    "RELEVANT_DIR = HAYSTACK_DIR / \"relevant\"\n",
    "IRRELAVANT_DIR = HAYSTACK_DIR / \"irrelevant\"\n",
    "\n",
    "sys.path.append(str(PROJECT_DIR))\n",
    "\n",
    "# Set Hugging Face cache path. Use Absolute path.\n",
    "os.environ['HF_HOME'] = '/cs/student/projects1/2021/aagarwal/SNLP_Project/.cache/hf_with_quota'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from argparse import Namespace\n",
    "\n",
    "def get_args(id, context_type, data):\n",
    "    \n",
    "    item = next(item for item in data if item[\"id\"] == id)\n",
    "\n",
    "    if context_type == \"relevant\":\n",
    "        haystack_file = RELEVANT_DIR / item[\"context_relevant\"]\n",
    "    elif context_type == \"irrelevant\":\n",
    "        haystack_file = IRRELAVANT_DIR / item[\"context_irrelevant\"]\n",
    "\n",
    "    args = Namespace(\n",
    "        model_name = \"yaofu/llama-2-7b-80k\",\n",
    "        model_name_suffix = f\"{context_type} + {item[\"id\"]}\",\n",
    "        model_provider = \"LLaMA\",\n",
    "\n",
    "        context_lengths_min = 0,\n",
    "        context_lengths_max = 5000,\n",
    "        context_lengths_num_intervals = 10,\n",
    "\n",
    "        document_depth_percent_intervals = 10,\n",
    "\n",
    "        needle = item[\"needle\"],\n",
    "        real_needle = item[\"real_needle\"],\n",
    "        retrieval_question = item[\"question\"],\n",
    "        haystack_file = haystack_file,\n",
    "    )\n",
    "\n",
    "    return args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading from yaofu/llama-2-7b-80k\n",
      "layer number: 32, head number 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation=\"flash_attention_2\"` instead.\n",
      "LlamaForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bb1ea84607c419fa0391d5570a31cbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from retrieval_head_detection import LLMNeedleHaystackTester as Tester\n",
    "\n",
    "with open(HAYSTACK_DIR / \"needles.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "args = get_args(id=44, context_type=\"relevant\")\n",
    "tester = Tester(**vars(args))\n",
    "\n",
    "tester.start_test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
