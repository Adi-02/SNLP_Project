{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Overview\n",
    "\n",
    "This notebook provides an faster testing interface. \n",
    "\n",
    "It includes steps to \n",
    " - Check GPU availability\n",
    " - Set up the environment, \n",
    " - Define helper functions,\n",
    " - Run tests with a given **id** and **context_type**. \n",
    "\n",
    "The results are saved for further analysis.\n",
    "\n",
    "*Make sure to select the created `venv` as your kernel.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check GPU Availability\n",
    "Run this cell to check if there is free space on the gpu. \n",
    "\n",
    "If the **free space** is not close to the **total space** then someone else is probably using the machine. \n",
    "\n",
    "You can check the active processes bu running `nvidia-smi` in the terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0:  1.631 GiB free /  23.99 GiB total\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "def get_gpu_memory():\n",
    "    result = subprocess.run(\n",
    "        ['nvidia-smi', '--query-gpu=memory.free,memory.total', '--format=csv,nounits,noheader'],\n",
    "        stdout=subprocess.PIPE, text=True\n",
    "    )\n",
    "    lines = result.stdout.strip().split('\\n')\n",
    "    for i, line in enumerate(lines):\n",
    "        free, total = map(int, line.split(','))\n",
    "        print(f\"GPU {i}: {free / 1024: .4} GiB free / {total / 1024: .4} GiB total\")\n",
    "\n",
    "get_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "The code below imports necessary modules, defines directory paths, and sets the Hugging Face cache path.\n",
    "\n",
    "**No need to modify this.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_DIR = Path.cwd()\n",
    "HAYSTACK_DIR = PROJECT_DIR / \"haystack\"\n",
    "RELEVANT_DIR = HAYSTACK_DIR / \"relevant\"\n",
    "IRRELAVANT_DIR = HAYSTACK_DIR / \"irrelevant\"\n",
    "\n",
    "sys.path.append(str(PROJECT_DIR))\n",
    "\n",
    "with open(HAYSTACK_DIR / \"needles.json\", \"r\") as f:\n",
    "    NEEDLES_DATA = json.load(f)\n",
    "\n",
    "# This sets the Hugging Face cache path. Make sure this directory exists. If not, refer to the README.\n",
    "os.environ['HF_HOME'] = '.cache/hf_with_quota'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "\n",
    "def get_args(id, context_type, data=NEEDLES_DATA):\n",
    "    \"\"\"\n",
    "    Get the arguments for the given id and context type.\n",
    "    Arguments:\n",
    "        id (int): The id of the item.\n",
    "        context_type (str): The type of context (\"relevant\" or \"irrelevant\").\n",
    "        data (list): The list of data items.\n",
    "    Returns:\n",
    "        Namespace: The arguments for the given id and context type.\n",
    "    \"\"\"\n",
    "    \n",
    "    # find the item with the given id\n",
    "    item = next(item for item in data if item[\"id\"] == id)\n",
    "\n",
    "    if context_type == \"relevant\":\n",
    "        haystack_file = RELEVANT_DIR / item[\"context_relevant\"]\n",
    "    elif context_type == \"irrelevant\":\n",
    "        haystack_file = IRRELAVANT_DIR / item[\"context_irrelevant\"]\n",
    "\n",
    "    args = Namespace(\n",
    "        model_name = \"yaofu/llama-2-7b-80k\",\n",
    "        model_name_suffix = f\"{context_type}_id_{item['id']}\", # suffix used to name the results files,\n",
    "        model_provider = \"LLaMA\",\n",
    "\n",
    "        context_lengths_min = 0, # min context length\n",
    "        context_lengths_max = 5000, # max context length\n",
    "        context_lengths_num_intervals = 10, # number of intervals for context lengths\n",
    "\n",
    "        document_depth_percent_intervals = 10, # number of intervals for document depth\n",
    "\n",
    "        needle = item[\"needle\"],\n",
    "        real_needle = item[\"real_needle\"],\n",
    "        retrieval_question = item[\"question\"],\n",
    "        haystack_file = haystack_file,\n",
    "    )\n",
    "\n",
    "    return args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runing the Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the test object:\n",
    "Specify the **id** and the **context_type** in the `get_args` function. \n",
    "\n",
    "The notebook currently supports only individual testing for relevant and irrelevant context. \n",
    "\n",
    "The code for testing with misleading context will be added soon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading from yaofu/llama-2-7b-80k\n",
      "layer number: 32, head number 32\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ade93e11a7245a8906c56cda927ae41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "from retrieval_head_detection import LLMNeedleHaystackTester as RetrievalHeads\n",
    "\n",
    "# Get the arguments for the given id and context type\n",
    "args = get_args(id=44, context_type=\"relevant\")\n",
    "\n",
    "# Create the Tester object.\n",
    "rh = RetrievalHeads(**vars(args))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the Test\n",
    "\n",
    "Running the test will save:\n",
    "\n",
    "- **Contexts** → Given to the model per context length/depth  \n",
    "  → `contexts/{model_name}_{context_type}_id_{id}`\n",
    "\n",
    "- **Results** → Model outputs per context length/depth  \n",
    "  → `results/graph/{model_name}_{context_type}_id_{id}`\n",
    "\n",
    "**Example**:  \n",
    "`results/graph/llama-2-7b-80k_relevant_id_44/`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Starting Needle In A Haystack Testing...\n",
      "- Model: yaofu/llama-2-7b-80k\n",
      "- Context Lengths: 10, Min: 0, Max: 5000\n",
      "- Document Depths: 10, Min: 0%, Max: 100%\n",
      "- Needle: There won't be a FIFA World Cup this year.\n",
      "\n",
      "\n",
      "\n",
      "insertion at 0\n",
      "There won't be a FIFA World Cup this year.\n",
      "[['16-19'], ['7-4'], ['11-15'], ['6-9'], ['19-15'], ['6-30'], ['8-26'], ['11-2'], ['12-26'], ['17-22'], ['21-30'], ['28-14'], ['19-12'], ['21-4'], ['24-11'], ['24-29'], ['24-30'], ['25-3'], ['29-21'], ['29-26']]\n",
      "-- Test Summary -- \n",
      "Duration: 14.1 seconds\n",
      "Context: 0 tokens\n",
      "Depth: 0%\n",
      "Score: 100.0\n",
      "Response: There won't be a FIFA World Cup this year.\n",
      "\n",
      "Writing at results/graph/llama-2-7b-80k_relevant_id_44/llama-2-7b-80k_relevant_id_44_len_0_depth_0_results.json\n",
      "insertion at 0\n",
      "There won't be a FIFA World Cup this year.\n",
      "[['16-19'], ['7-4'], ['11-15'], ['6-9'], ['19-15'], ['6-30'], ['8-26'], ['11-2'], ['12-26'], ['17-22'], ['21-30'], ['28-14'], ['19-12'], ['21-4'], ['24-11'], ['24-29'], ['24-30'], ['25-3'], ['29-21'], ['29-26']]\n",
      "-- Test Summary -- \n",
      "Duration: 13.5 seconds\n",
      "Context: 0 tokens\n",
      "Depth: 11%\n",
      "Score: 100.0\n",
      "Response: There won't be a FIFA World Cup this year.\n",
      "\n",
      "Writing at results/graph/llama-2-7b-80k_relevant_id_44/llama-2-7b-80k_relevant_id_44_len_0_depth_1100_results.json\n",
      "insertion at 0\n",
      "There won't be a FIFA World Cup this year.\n",
      "[['16-19'], ['7-4'], ['11-15'], ['6-9'], ['19-15'], ['6-30'], ['8-26'], ['11-2'], ['12-26'], ['17-22'], ['21-30'], ['28-14'], ['19-12'], ['21-4'], ['24-11'], ['24-29'], ['24-30'], ['25-3'], ['29-21'], ['29-26']]\n",
      "-- Test Summary -- \n",
      "Duration: 13.5 seconds\n",
      "Context: 0 tokens\n",
      "Depth: 22%\n",
      "Score: 100.0\n",
      "Response: There won't be a FIFA World Cup this year.\n",
      "\n",
      "Writing at results/graph/llama-2-7b-80k_relevant_id_44/llama-2-7b-80k_relevant_id_44_len_0_depth_2200_results.json\n",
      "insertion at 0\n",
      "There won't be a FIFA World Cup this year.\n",
      "[['16-19'], ['7-4'], ['11-15'], ['6-9'], ['19-15'], ['6-30'], ['8-26'], ['11-2'], ['12-26'], ['17-22'], ['21-30'], ['28-14'], ['19-12'], ['21-4'], ['24-11'], ['24-29'], ['24-30'], ['25-3'], ['29-21'], ['29-26']]\n",
      "-- Test Summary -- \n",
      "Duration: 13.6 seconds\n",
      "Context: 0 tokens\n",
      "Depth: 33%\n",
      "Score: 100.0\n",
      "Response: There won't be a FIFA World Cup this year.\n",
      "\n",
      "Writing at results/graph/llama-2-7b-80k_relevant_id_44/llama-2-7b-80k_relevant_id_44_len_0_depth_3300_results.json\n",
      "insertion at 0\n",
      "There won't be a FIFA World Cup this year.\n",
      "[['16-19'], ['7-4'], ['11-15'], ['6-9'], ['19-15'], ['6-30'], ['8-26'], ['11-2'], ['12-26'], ['17-22'], ['21-30'], ['28-14'], ['19-12'], ['21-4'], ['24-11'], ['24-29'], ['24-30'], ['25-3'], ['29-21'], ['29-26']]\n",
      "-- Test Summary -- \n",
      "Duration: 13.6 seconds\n",
      "Context: 0 tokens\n",
      "Depth: 44%\n",
      "Score: 100.0\n",
      "Response: There won't be a FIFA World Cup this year.\n",
      "\n",
      "Writing at results/graph/llama-2-7b-80k_relevant_id_44/llama-2-7b-80k_relevant_id_44_len_0_depth_4400_results.json\n",
      "insertion at 0\n",
      "There won't be a FIFA World Cup this year.\n",
      "[['16-19'], ['7-4'], ['11-15'], ['6-9'], ['19-15'], ['6-30'], ['8-26'], ['11-2'], ['12-26'], ['17-22'], ['21-30'], ['28-14'], ['19-12'], ['21-4'], ['24-11'], ['24-29'], ['24-30'], ['25-3'], ['29-21'], ['29-26']]\n",
      "-- Test Summary -- \n",
      "Duration: 13.6 seconds\n",
      "Context: 0 tokens\n",
      "Depth: 56%\n",
      "Score: 100.0\n",
      "Response: There won't be a FIFA World Cup this year.\n",
      "\n",
      "Writing at results/graph/llama-2-7b-80k_relevant_id_44/llama-2-7b-80k_relevant_id_44_len_0_depth_5600_results.json\n",
      "insertion at 0\n",
      "There won't be a FIFA World Cup this year.\n",
      "[['16-19'], ['7-4'], ['11-15'], ['6-9'], ['19-15'], ['6-30'], ['8-26'], ['11-2'], ['12-26'], ['17-22'], ['21-30'], ['28-14'], ['19-12'], ['21-4'], ['24-11'], ['24-29'], ['24-30'], ['25-3'], ['29-21'], ['29-26']]\n",
      "-- Test Summary -- \n",
      "Duration: 13.6 seconds\n",
      "Context: 0 tokens\n",
      "Depth: 67%\n",
      "Score: 100.0\n",
      "Response: There won't be a FIFA World Cup this year.\n",
      "\n",
      "Writing at results/graph/llama-2-7b-80k_relevant_id_44/llama-2-7b-80k_relevant_id_44_len_0_depth_6700_results.json\n",
      "insertion at 0\n",
      "There won't be a FIFA World Cup this year.\n",
      "[['16-19'], ['7-4'], ['11-15'], ['6-9'], ['19-15'], ['6-30'], ['8-26'], ['11-2'], ['12-26'], ['17-22'], ['21-30'], ['28-14'], ['19-12'], ['21-4'], ['24-11'], ['24-29'], ['24-30'], ['25-3'], ['29-21'], ['29-26']]\n",
      "-- Test Summary -- \n",
      "Duration: 13.6 seconds\n",
      "Context: 0 tokens\n",
      "Depth: 78%\n",
      "Score: 100.0\n",
      "Response: There won't be a FIFA World Cup this year.\n",
      "\n",
      "Writing at results/graph/llama-2-7b-80k_relevant_id_44/llama-2-7b-80k_relevant_id_44_len_0_depth_7800_results.json\n",
      "insertion at 0\n",
      "There won't be a FIFA World Cup this year.\n",
      "[['16-19'], ['7-4'], ['11-15'], ['6-9'], ['19-15'], ['6-30'], ['8-26'], ['11-2'], ['12-26'], ['17-22'], ['21-30'], ['28-14'], ['19-12'], ['21-4'], ['24-11'], ['24-29'], ['24-30'], ['25-3'], ['29-21'], ['29-26']]\n",
      "-- Test Summary -- \n",
      "Duration: 13.6 seconds\n",
      "Context: 0 tokens\n",
      "Depth: 89%\n",
      "Score: 100.0\n",
      "Response: There won't be a FIFA World Cup this year.\n",
      "\n",
      "Writing at results/graph/llama-2-7b-80k_relevant_id_44/llama-2-7b-80k_relevant_id_44_len_0_depth_8900_results.json\n",
      "There won't be a FIFA World Cup this year.\n"
     ]
    }
   ],
   "source": [
    "rh.start_test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
