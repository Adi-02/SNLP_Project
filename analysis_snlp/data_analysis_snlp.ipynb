{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview:\n",
    "\n",
    "This notebook processes and analyzes model test result JSON files to extract unique model responses, their scores and frequencies, and save the results for different context types. It includes functions to:\n",
    "\n",
    "- Load and process JSON files for a given context and ID.\n",
    "- Extract unique responses, their scores, and frequencies.\n",
    "- Save the processed data for all context types into a single JSON file for a specified ID.\n",
    "\n",
    "Additionally, it demonstrates saving the results for a specific ID as an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and Process JSON Files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import tqdm\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import time\n",
    "from groq import Groq\n",
    "\n",
    "\n",
    "\n",
    "def __get_unique_model_responses(id: int, context_type: str, with_misleading: bool = False) -> list:\n",
    "    \"\"\"\n",
    "    Load and process model test result JSON files for a given context and ID.\n",
    "\n",
    "    Returns a list of unique model responses with their:\n",
    "    - score (from first occurrence)\n",
    "    - frequency (how many times that response appeared)\n",
    "    \n",
    "    Sorted by descending score.\n",
    "    \"\"\"\n",
    "    model_name = 'llama-2-7b-80k'\n",
    "    base_dir = Path('../results/graph')\n",
    "    \n",
    "    suffix = '_misleading' if with_misleading else ''\n",
    "    directory = base_dir / f'{model_name}_id_{id}_{context_type}{suffix}'\n",
    "\n",
    "    # Collect all responses and scores\n",
    "    data = [\n",
    "        json.load(open(directory / file, encoding='utf-8'))\n",
    "        for file in os.listdir(directory) if file.endswith('.json')\n",
    "    ]\n",
    "    \n",
    "    df = pd.DataFrame(data)[['model_response', 'needle', 'context_length', 'depth_percent']]\n",
    "    question_df = pd.read_excel(\"../data_generation/FreshQADataset_with_misleading.xlsx\")\n",
    "    try:\n",
    "        question = question_df.set_index(\"id\").at[id, \"question\"]\n",
    "        df['question'] = question\n",
    "        \n",
    "    except KeyError:\n",
    "        raise ValueError(f\"No question found for id: {id}\")\n",
    "\n",
    "    # Compute frequency\n",
    "    freq = df['model_response'].value_counts().rename('frequency')\n",
    "\n",
    "    # Drop duplicates but keep first score\n",
    "    # df = df.drop_duplicates(subset='model_response')\n",
    "\n",
    "    # Merge with frequency\n",
    "    df = df.merge(freq, left_on='model_response', right_index=True)\n",
    "\n",
    "    # Sort by score descending\n",
    "    # df = df.sort_values(by='score', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    return df.to_dict(orient='records')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Processed Results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_all_contexts_for_id(id: int) -> None:\n",
    "    \"\"\"\n",
    "    Gather results from all 4 context types and save them in a single JSON file for the given id.\n",
    "\n",
    "    File will be saved as: unique_responses/{id}.json\n",
    "    \"\"\"\n",
    "    save_dir = Path('unique_responses')\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    all_data = {\n",
    "        'relevant': __get_unique_model_responses(id, 'relevant', with_misleading=False),\n",
    "        'relevant_misleading': __get_unique_model_responses(id, 'relevant', with_misleading=True),\n",
    "        'irrelevant': __get_unique_model_responses(id, 'irrelevant', with_misleading=False),\n",
    "        'irrelevant_misleading': __get_unique_model_responses(id, 'irrelevant', with_misleading=True),\n",
    "    }\n",
    "\n",
    "    save_path = save_dir / f'{id}.json'\n",
    "    with open(save_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(all_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"Saved all context responses for ID {id} to {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_val = 44"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['model_response', 'needle', 'context_length', 'depth_percent'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43msave_all_contexts_for_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43mid_val\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36msave_all_contexts_for_id\u001b[39m\u001b[34m(id)\u001b[39m\n\u001b[32m      7\u001b[39m save_dir = Path(\u001b[33m'\u001b[39m\u001b[33munique_responses\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      8\u001b[39m save_dir.mkdir(parents=\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     10\u001b[39m all_data = {\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mrelevant\u001b[39m\u001b[33m'\u001b[39m: \u001b[43m__get_unique_model_responses\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrelevant\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwith_misleading\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m,\n\u001b[32m     12\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mrelevant_misleading\u001b[39m\u001b[33m'\u001b[39m: __get_unique_model_responses(\u001b[38;5;28mid\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mrelevant\u001b[39m\u001b[33m'\u001b[39m, with_misleading=\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[32m     13\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mirrelevant\u001b[39m\u001b[33m'\u001b[39m: __get_unique_model_responses(\u001b[38;5;28mid\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mirrelevant\u001b[39m\u001b[33m'\u001b[39m, with_misleading=\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[32m     14\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mirrelevant_misleading\u001b[39m\u001b[33m'\u001b[39m: __get_unique_model_responses(\u001b[38;5;28mid\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mirrelevant\u001b[39m\u001b[33m'\u001b[39m, with_misleading=\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[32m     15\u001b[39m }\n\u001b[32m     17\u001b[39m save_path = save_dir / \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mid\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.json\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(save_path, \u001b[33m'\u001b[39m\u001b[33mw\u001b[39m\u001b[33m'\u001b[39m, encoding=\u001b[33m'\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 36\u001b[39m, in \u001b[36m__get_unique_model_responses\u001b[39m\u001b[34m(id, context_type, with_misleading)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# Collect all responses and scores\u001b[39;00m\n\u001b[32m     31\u001b[39m data = [\n\u001b[32m     32\u001b[39m     json.load(\u001b[38;5;28mopen\u001b[39m(directory / file, encoding=\u001b[33m'\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m     33\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m os.listdir(directory) \u001b[38;5;28;01mif\u001b[39;00m file.endswith(\u001b[33m'\u001b[39m\u001b[33m.json\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     34\u001b[39m ]\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmodel_response\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mneedle\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcontext_length\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdepth_percent\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     37\u001b[39m question_df = pd.read_excel(\u001b[33m\"\u001b[39m\u001b[33m../data_generation/FreshQADataset_with_misleading.xlsx\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aadhi\\Desktop\\University 4th Year\\Statistical NLP\\SNLP_Project\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:4108\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4106\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[32m   4107\u001b[39m         key = \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[32m-> \u001b[39m\u001b[32m4108\u001b[39m     indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcolumns\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m1\u001b[39m]\n\u001b[32m   4110\u001b[39m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[32m   4111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == \u001b[38;5;28mbool\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aadhi\\Desktop\\University 4th Year\\Statistical NLP\\SNLP_Project\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[39m, in \u001b[36mIndex._get_indexer_strict\u001b[39m\u001b[34m(self, key, axis_name)\u001b[39m\n\u001b[32m   6197\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6198\u001b[39m     keyarr, indexer, new_indexer = \u001b[38;5;28mself\u001b[39m._reindex_non_unique(keyarr)\n\u001b[32m-> \u001b[39m\u001b[32m6200\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6202\u001b[39m keyarr = \u001b[38;5;28mself\u001b[39m.take(indexer)\n\u001b[32m   6203\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[32m   6204\u001b[39m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aadhi\\Desktop\\University 4th Year\\Statistical NLP\\SNLP_Project\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6249\u001b[39m, in \u001b[36mIndex._raise_if_missing\u001b[39m\u001b[34m(self, key, indexer, axis_name)\u001b[39m\n\u001b[32m   6247\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m nmissing:\n\u001b[32m   6248\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m nmissing == \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[32m-> \u001b[39m\u001b[32m6249\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   6251\u001b[39m     not_found = \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask.nonzero()[\u001b[32m0\u001b[39m]].unique())\n\u001b[32m   6252\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not in index\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: \"None of [Index(['model_response', 'needle', 'context_length', 'depth_percent'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "save_all_contexts_for_id(id_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'model_response': 'No, the FIFA World Cup will not be hosted this year.',\n",
       "  'score': 68.16167831420898,\n",
       "  'needle': \"There won't be a FIFA World Cup this year.\",\n",
       "  'frequency': 10},\n",
       " {'model_response': 'No, the FIFA World Cup will not be hosted this year. The FIFA World Cup is a major international football tournament that is held every four years. The tournament is organized by FIFA, the international governing body of football, and is contested by',\n",
       "  'score': 64.01805877685547,\n",
       "  'needle': \"There won't be a FIFA World Cup this year.\",\n",
       "  'frequency': 1},\n",
       " {'model_response': \"No, the FIFA World Cup will not be hosted this year. The FIFA World Cup is a major international football tournament that is held every four years. The tournament is organized by the Fédération Internationale de Football Association (FIFA), the sport'\",\n",
       "  'score': 63.443970680236816,\n",
       "  'needle': \"There won't be a FIFA World Cup this year.\",\n",
       "  'frequency': 1},\n",
       " {'model_response': 'No, the FIFA World Cup will not be hosted this year. The FIFA World Cup is a soccer tournament that is held every four years. The next FIFA World Cup will be held in 2022.',\n",
       "  'score': 61.14392280578613,\n",
       "  'needle': \"There won't be a FIFA World Cup this year.\",\n",
       "  'frequency': 1},\n",
       " {'model_response': 'No. The FIFA World Cup is a quadrennial event. It is held every four years. The next FIFA World Cup will be held in 2022.',\n",
       "  'score': 52.48445272445679,\n",
       "  'needle': \"There won't be a FIFA World Cup this year.\",\n",
       "  'frequency': 1},\n",
       " {'model_response': 'No. The 1942 and 1946 competitions, which Germany and Brazil sought to host, were cancelled due to World War II.',\n",
       "  'score': 42.26422905921936,\n",
       "  'needle': \"There won't be a FIFA World Cup this year.\",\n",
       "  'frequency': 1},\n",
       " {'model_response': 'No.',\n",
       "  'score': 11.441656947135925,\n",
       "  'needle': \"There won't be a FIFA World Cup this year.\",\n",
       "  'frequency': 10}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "__get_unique_model_responses(id_val, 'relevant', with_misleading=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rewrite_prompt(model_response, real_needle, question):\n",
    "    return (\n",
    "        f\"\"\"You are evaluating a model-generated response for hallucination analysis in a long-context retrieval setting.\n",
    "\n",
    "            You are given the following:\n",
    "\n",
    "            Question: {question}\n",
    "\n",
    "            The correct answer (real needle): {real_needle}\n",
    "\n",
    "            A model-generated response: {model_response}\n",
    "\n",
    "            Your task is to classify the model's response into one of the following categories:\n",
    "\n",
    "            1. Correct Retrieval: the model answered the question correctly\n",
    "\n",
    "            2. Excess Incorrect Retrieval: the model answered the question correctly but it also went on to output other text that was either completely irrelevant or invalid with regards to the question.\n",
    "\n",
    "            3. Irrelevant Retrieval: the model did not answer the question, it went on to output other text\n",
    "\n",
    "            4. Contradictory Retrieval: the model answered the question incorrectly\n",
    "\n",
    "            Examples:\n",
    "\n",
    "            Question: Who is the President of the United States? Needle: The President of the United States is Donald Trump.\n",
    "\n",
    "            1: The President of the United States is Donald Trump. (Correct Retrieval)\n",
    "\n",
    "            2: The President of the United States is Donald Trump. The affirmation was made in the Declaration of Independence, which was written predominantly by Thomas Jefferson in 1776. (Excess Incorrect Retrieval)\n",
    "\n",
    "            3: The President of the United States is the head of state and head of government of the United States. The president directs the executive branch of the federal government and is the commander-in-chief of the United States Armed Forces. (Irrelevant Retrieval)\n",
    "\n",
    "            4: Joe Biden is the President of the United States. (Contradictory Retrieval)\n",
    "\n",
    "            Return your output in this exact JSON format: {{\"model_response\": \"<text>\", \"needle\": \"<real_needle>\", \"hallucination_type\": \"<Correct Retrieval | Excess Incorrect Retrieval | Irrelevant Retrieval | Contradictory Retrieval>\"}}\"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load .env and API key\n",
    "load_dotenv()\n",
    "groq_api_key = os.getenv(\"snlp_api_key\")\n",
    "\n",
    "# Initialize Groq client\n",
    "client = Groq(api_key=groq_api_key)\n",
    "\n",
    "def rewrite_question_and_needle(question, needle):\n",
    "    while True:\n",
    "        try:\n",
    "            prompt = build_rewrite_prompt(question, needle)\n",
    "            completion = client.chat.completions.create(\n",
    "                model=\"llama-3.3-70b-versatile\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=0.7,\n",
    "                max_tokens=512,\n",
    "                top_p=1.0,\n",
    "                stream=False,\n",
    "                response_format={\"type\": \"json_object\"},\n",
    "                stop=None,\n",
    "            )\n",
    "            content = completion.choices[0].message.content\n",
    "            data = json.loads(content)\n",
    "            return data[\"hallucination_type\"]\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            print(\"Retrying now...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       model_response      score  \\\n",
      "0   No, the FIFA World Cup will not be hosted this...  68.161678   \n",
      "1   No, the FIFA World Cup will not be hosted this...  64.018059   \n",
      "2   No, the FIFA World Cup will not be hosted this...  63.443971   \n",
      "3   No, the FIFA World Cup will not be hosted this...  61.143923   \n",
      "4   No. The FIFA World Cup is a quadrennial event....  52.484453   \n",
      "5   No. The 1942 and 1946 competitions, which Germ...  42.264229   \n",
      "6                                                 No.  11.441657   \n",
      "7      No, there won't be a FIFA World Cup this year.  84.071064   \n",
      "8   No, the FIFA World Cup will not be hosted this...  68.161678   \n",
      "9   No, the FIFA World Cup will not be hosted this...  64.754856   \n",
      "10  No, the FIFA World Cup will not be hosted this...  60.974330   \n",
      "11                                                No.  11.441657   \n",
      "12                                      No, it won't.  10.532115   \n",
      "13  No, the FIFA World Cup will not be held this y...  71.780235   \n",
      "14  No, the FIFA World Cup will not be hosted this...  68.161678   \n",
      "15  No, the FIFA World Cup will not be hosted this...  64.018059   \n",
      "16                                                No.  11.441657   \n",
      "17  No, the FIFA World Cup will not be held this y...  71.780235   \n",
      "18  No, there won't be a FIFA World Cup this year....  68.461436   \n",
      "19  No, the FIFA World Cup will not be hosted this...  68.161678   \n",
      "20                     No, it will be held next year.  42.366514   \n",
      "21                                                No.  11.441657   \n",
      "22                                      No, it won't.  10.532115   \n",
      "\n",
      "                                        needle  frequency  \\\n",
      "0   There won't be a FIFA World Cup this year.         10   \n",
      "1   There won't be a FIFA World Cup this year.          1   \n",
      "2   There won't be a FIFA World Cup this year.          1   \n",
      "3   There won't be a FIFA World Cup this year.          1   \n",
      "4   There won't be a FIFA World Cup this year.          1   \n",
      "5   There won't be a FIFA World Cup this year.          1   \n",
      "6   There won't be a FIFA World Cup this year.         10   \n",
      "7   There won't be a FIFA World Cup this year.          1   \n",
      "8   There won't be a FIFA World Cup this year.         18   \n",
      "9   There won't be a FIFA World Cup this year.          1   \n",
      "10  There won't be a FIFA World Cup this year.          1   \n",
      "11  There won't be a FIFA World Cup this year.          1   \n",
      "12  There won't be a FIFA World Cup this year.          3   \n",
      "13  There won't be a FIFA World Cup this year.          1   \n",
      "14  There won't be a FIFA World Cup this year.         14   \n",
      "15  There won't be a FIFA World Cup this year.          1   \n",
      "16  There won't be a FIFA World Cup this year.          9   \n",
      "17  There won't be a FIFA World Cup this year.          1   \n",
      "18  There won't be a FIFA World Cup this year.          1   \n",
      "19  There won't be a FIFA World Cup this year.         18   \n",
      "20  There won't be a FIFA World Cup this year.          1   \n",
      "21  There won't be a FIFA World Cup this year.          3   \n",
      "22  There won't be a FIFA World Cup this year.          1   \n",
      "\n",
      "                 category  \n",
      "0                relevant  \n",
      "1                relevant  \n",
      "2                relevant  \n",
      "3                relevant  \n",
      "4                relevant  \n",
      "5                relevant  \n",
      "6                relevant  \n",
      "7     relevant_misleading  \n",
      "8     relevant_misleading  \n",
      "9     relevant_misleading  \n",
      "10    relevant_misleading  \n",
      "11    relevant_misleading  \n",
      "12    relevant_misleading  \n",
      "13             irrelevant  \n",
      "14             irrelevant  \n",
      "15             irrelevant  \n",
      "16             irrelevant  \n",
      "17  irrelevant_misleading  \n",
      "18  irrelevant_misleading  \n",
      "19  irrelevant_misleading  \n",
      "20  irrelevant_misleading  \n",
      "21  irrelevant_misleading  \n",
      "22  irrelevant_misleading  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open(f\"unique_responses/{id_val}.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "rows = []\n",
    "for category, entries in data.items():\n",
    "    for entry in entries:\n",
    "        entry[\"category\"] = category\n",
    "        rows.append(entry)\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Refining Q&A pairs:   0%|          | 0/23 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Rate limit pause: sleeping for 15 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Refining Q&A pairs:  74%|███████▍  | 17/23 [00:17<00:03,  1.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Rate limit pause: sleeping for 15 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Refining Q&A pairs: 100%|██████████| 23/23 [00:33<00:00,  1.45s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "hallucination_types = []\n",
    "\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Refining Q&A pairs\"):\n",
    "    mr, n, q = row.get(\"model_response\", \"\"), row.get(\"needle\", \"\"), row.get(\"question\", \"\")\n",
    "    # if s > 50:\n",
    "    #     current_hallucination_type = \"No Hallucination\"\n",
    "    \n",
    "    # else:\n",
    "    if idx > 0 and idx % 5 == 0:\n",
    "        print(\"⏳ Rate limit pause: sleeping for 15 seconds...\")\n",
    "        time.sleep(15)\n",
    "        \n",
    "    current_hallucination_type = rewrite_question_and_needle(mr, n, q)\n",
    "    hallucination_types.append(current_hallucination_type)\n",
    "\n",
    "# Update the DataFrame\n",
    "df['hallucination_type'] = hallucination_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by category and convert each group to a list of dicts\n",
    "json_output = {\n",
    "    category: group.drop(columns=\"category\").to_dict(orient=\"records\")\n",
    "    for category, group in df.groupby(\"category\")\n",
    "}\n",
    "\n",
    "output_dir = \"updated_unique_responses\"\n",
    "file_id = str(id_val)  # or whatever variable you're using\n",
    "output_path = os.path.join(output_dir, f\"{file_id}.json\")\n",
    "\n",
    "# Create the folder if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Write the JSON file into the folder\n",
    "with open(output_path, \"w\") as f:\n",
    "    json.dump(json_output, f, indent=4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
