{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview:\n",
    "\n",
    "This notebook processes and analyzes model test result JSON files to extract unique model responses, their scores and frequencies, and save the results for different context types. It includes functions to:\n",
    "\n",
    "- Load and process JSON files for a given context and ID.\n",
    "- Extract unique responses, their scores, and frequencies.\n",
    "- Save the processed data for all context types into a single JSON file for a specified ID.\n",
    "\n",
    "Additionally, it demonstrates saving the results for a specific ID as an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and Process JSON Files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import tqdm\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import time\n",
    "from groq import Groq\n",
    "\n",
    "\n",
    "\n",
    "def __get_unique_model_responses(id: int, context_type: str, with_misleading: bool = False) -> list:\n",
    "    \"\"\"\n",
    "    Load and process model test result JSON files for a given context and ID.\n",
    "\n",
    "    Returns a list of unique model responses with their:\n",
    "    - score (from first occurrence)\n",
    "    - frequency (how many times that response appeared)\n",
    "    \n",
    "    Sorted by descending score.\n",
    "    \"\"\"\n",
    "    model_name = 'llama-2-7b-80k'\n",
    "    base_dir = Path('../results/graph')\n",
    "    \n",
    "    suffix = '_misleading' if with_misleading else ''\n",
    "    directory = base_dir / f'{model_name}_id_{id}_{context_type}{suffix}'\n",
    "\n",
    "    # Collect all responses and scores\n",
    "    data = [\n",
    "        json.load(open(directory / file, encoding='utf-8'))\n",
    "        for file in os.listdir(directory) if file.endswith('.json')\n",
    "    ]\n",
    "    \n",
    "    df = pd.DataFrame(data)[['model_response', 'needle', 'context_length', 'depth_percent']]\n",
    "    question_df = pd.read_excel(\"../data_generation/FreshQADataset_with_misleading.xlsx\")\n",
    "    try:\n",
    "        question = question_df.set_index(\"id\").at[id, \"question\"]\n",
    "        df['question'] = question\n",
    "        \n",
    "    except KeyError:\n",
    "        raise ValueError(f\"No question found for id: {id}\")\n",
    "\n",
    "    # Compute frequency\n",
    "    freq = df['model_response'].value_counts().rename('frequency')\n",
    "\n",
    "    # Drop duplicates but keep first score\n",
    "    # df = df.drop_duplicates(subset='model_response')\n",
    "\n",
    "    # Merge with frequency\n",
    "    df = df.merge(freq, left_on='model_response', right_index=True)\n",
    "\n",
    "    # Sort by score descending\n",
    "    # df = df.sort_values(by='score', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    return df.to_dict(orient='records')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Processed Results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_all_contexts_for_id(id: int) -> None:\n",
    "    \"\"\"\n",
    "    Gather results from all 4 context types and save them in a single JSON file for the given id.\n",
    "\n",
    "    File will be saved as: unique_responses/{id}.json\n",
    "    \"\"\"\n",
    "    save_dir = Path('unique_responses')\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    all_data = {\n",
    "        'relevant': __get_unique_model_responses(id, 'relevant', with_misleading=False),\n",
    "        'relevant_misleading': __get_unique_model_responses(id, 'relevant', with_misleading=True),\n",
    "        'irrelevant': __get_unique_model_responses(id, 'irrelevant', with_misleading=False),\n",
    "        'irrelevant_misleading': __get_unique_model_responses(id, 'irrelevant', with_misleading=True),\n",
    "    }\n",
    "\n",
    "    save_path = save_dir / f'{id}.json'\n",
    "    with open(save_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(all_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"Saved all context responses for ID {id} to {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_val = 44"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: '..\\\\results\\\\graph\\\\llama-2-7b-80k_id_44_relevant'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43msave_all_contexts_for_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43mid_val\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36msave_all_contexts_for_id\u001b[39m\u001b[34m(id)\u001b[39m\n\u001b[32m      7\u001b[39m save_dir = Path(\u001b[33m'\u001b[39m\u001b[33munique_responses\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      8\u001b[39m save_dir.mkdir(parents=\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     10\u001b[39m all_data = {\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mrelevant\u001b[39m\u001b[33m'\u001b[39m: \u001b[43m__get_unique_model_responses\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrelevant\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwith_misleading\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m,\n\u001b[32m     12\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mrelevant_misleading\u001b[39m\u001b[33m'\u001b[39m: __get_unique_model_responses(\u001b[38;5;28mid\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mrelevant\u001b[39m\u001b[33m'\u001b[39m, with_misleading=\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[32m     13\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mirrelevant\u001b[39m\u001b[33m'\u001b[39m: __get_unique_model_responses(\u001b[38;5;28mid\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mirrelevant\u001b[39m\u001b[33m'\u001b[39m, with_misleading=\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[32m     14\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mirrelevant_misleading\u001b[39m\u001b[33m'\u001b[39m: __get_unique_model_responses(\u001b[38;5;28mid\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mirrelevant\u001b[39m\u001b[33m'\u001b[39m, with_misleading=\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[32m     15\u001b[39m }\n\u001b[32m     17\u001b[39m save_path = save_dir / \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mid\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.json\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(save_path, \u001b[33m'\u001b[39m\u001b[33mw\u001b[39m\u001b[33m'\u001b[39m, encoding=\u001b[33m'\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36m__get_unique_model_responses\u001b[39m\u001b[34m(id, context_type, with_misleading)\u001b[39m\n\u001b[32m     27\u001b[39m directory = base_dir / \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_id_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mid\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msuffix\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Collect all responses and scores\u001b[39;00m\n\u001b[32m     30\u001b[39m data = [\n\u001b[32m     31\u001b[39m     json.load(\u001b[38;5;28mopen\u001b[39m(directory / file, encoding=\u001b[33m'\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m file.endswith(\u001b[33m'\u001b[39m\u001b[33m.json\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     33\u001b[39m ]\n\u001b[32m     35\u001b[39m df = pd.DataFrame(data)[[\u001b[33m'\u001b[39m\u001b[33mmodel_response\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mneedle\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mcontext_length\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mdepth_percent\u001b[39m\u001b[33m'\u001b[39m]]\n\u001b[32m     36\u001b[39m question_df = pd.read_excel(\u001b[33m\"\u001b[39m\u001b[33m../data_generation/FreshQADataset_with_misleading.xlsx\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [WinError 3] The system cannot find the path specified: '..\\\\results\\\\graph\\\\llama-2-7b-80k_id_44_relevant'"
     ]
    }
   ],
   "source": [
    "save_all_contexts_for_id(id_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: '..\\\\results\\\\graph\\\\llama-2-7b-80k_id_44_relevant'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43m__get_unique_model_responses\u001b[49m\u001b[43m(\u001b[49m\u001b[43mid_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrelevant\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwith_misleading\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36m__get_unique_model_responses\u001b[39m\u001b[34m(id, context_type, with_misleading)\u001b[39m\n\u001b[32m     27\u001b[39m directory = base_dir / \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_id_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mid\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msuffix\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Collect all responses and scores\u001b[39;00m\n\u001b[32m     30\u001b[39m data = [\n\u001b[32m     31\u001b[39m     json.load(\u001b[38;5;28mopen\u001b[39m(directory / file, encoding=\u001b[33m'\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m file.endswith(\u001b[33m'\u001b[39m\u001b[33m.json\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     33\u001b[39m ]\n\u001b[32m     35\u001b[39m df = pd.DataFrame(data)[[\u001b[33m'\u001b[39m\u001b[33mmodel_response\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mneedle\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mcontext_length\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mdepth_percent\u001b[39m\u001b[33m'\u001b[39m]]\n\u001b[32m     36\u001b[39m question_df = pd.read_excel(\u001b[33m\"\u001b[39m\u001b[33m../data_generation/FreshQADataset_with_misleading.xlsx\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [WinError 3] The system cannot find the path specified: '..\\\\results\\\\graph\\\\llama-2-7b-80k_id_44_relevant'"
     ]
    }
   ],
   "source": [
    "__get_unique_model_responses(id_val, 'relevant', with_misleading=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rewrite_prompt(model_response, real_needle, question):\n",
    "    return (\n",
    "        f\"\"\"You are evaluating a model-generated response for hallucination analysis in a long-context retrieval setting.\n",
    "\n",
    "            You are given the following:\n",
    "\n",
    "            Question: {question}\n",
    "\n",
    "            The correct answer (real needle): {real_needle}\n",
    "\n",
    "            A model-generated response: {model_response}\n",
    "\n",
    "            Your task is to classify the model's response into one of the following categories:\n",
    "\n",
    "            1. Correct Retrieval: the model answered the question correctly\n",
    "\n",
    "            2. Excess Incorrect Retrieval: the model answered the question correctly but it also went on to output other text that was either completely irrelevant or invalid with regards to the question.\n",
    "\n",
    "            3. Irrelevant Retrieval: the model did not answer the question, it went on to output other text\n",
    "\n",
    "            4. Contradictory Retrieval: the model answered the question incorrectly\n",
    "\n",
    "            Examples:\n",
    "\n",
    "            Question: Who is the President of the United States? Needle: The President of the United States is Donald Trump.\n",
    "\n",
    "            1: The President of the United States is Donald Trump. (Correct Retrieval)\n",
    "\n",
    "            2: The President of the United States is Donald Trump. The affirmation was made in the Declaration of Independence, which was written predominantly by Thomas Jefferson in 1776. (Excess Incorrect Retrieval)\n",
    "\n",
    "            3: The President of the United States is the head of state and head of government of the United States. The president directs the executive branch of the federal government and is the commander-in-chief of the United States Armed Forces. (Irrelevant Retrieval)\n",
    "\n",
    "            4: Joe Biden is the President of the United States. (Contradictory Retrieval)\n",
    "\n",
    "            Return your output in this exact JSON format: {{\"model_response\": \"<text>\", \"needle\": \"<real_needle>\", \"hallucination_type\": \"<Correct Retrieval | Excess Incorrect Retrieval | Irrelevant Retrieval | Contradictory Retrieval>\"}}\"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "GroqError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the GROQ_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mGroqError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m groq_api_key = os.getenv(\u001b[33m\"\u001b[39m\u001b[33msnlp_api_key\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Initialize Groq client\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m client = \u001b[43mGroq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroq_api_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrewrite_question_and_needle\u001b[39m(response, needle, question):\n\u001b[32m      9\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aadhi\\Desktop\\University 4th Year\\Statistical NLP\\SNLP_Project\\.venv\\Lib\\site-packages\\groq\\_client.py:84\u001b[39m, in \u001b[36mGroq.__init__\u001b[39m\u001b[34m(self, api_key, base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[39m\n\u001b[32m     82\u001b[39m     api_key = os.environ.get(\u001b[33m\"\u001b[39m\u001b[33mGROQ_API_KEY\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m GroqError(\n\u001b[32m     85\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe api_key client option must be set either by passing api_key to the client or by setting the GROQ_API_KEY environment variable\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     86\u001b[39m     )\n\u001b[32m     87\u001b[39m \u001b[38;5;28mself\u001b[39m.api_key = api_key\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m base_url \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mGroqError\u001b[39m: The api_key client option must be set either by passing api_key to the client or by setting the GROQ_API_KEY environment variable"
     ]
    }
   ],
   "source": [
    "# Load .env and API key\n",
    "load_dotenv()\n",
    "groq_api_key = os.getenv(\"snlp_api_key\")\n",
    "\n",
    "# Initialize Groq client\n",
    "client = Groq(api_key=groq_api_key)\n",
    "\n",
    "def rewrite_question_and_needle(response, needle, question):\n",
    "    while True:\n",
    "        try:\n",
    "            prompt = build_rewrite_prompt(response, needle, question)\n",
    "            completion = client.chat.completions.create(\n",
    "                model=\"llama-3.3-70b-versatile\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=0.7,\n",
    "                max_tokens=512,\n",
    "                top_p=1.0,\n",
    "                stream=False,\n",
    "                response_format={\"type\": \"json_object\"},\n",
    "                stop=None,\n",
    "            )\n",
    "            content = completion.choices[0].message.content\n",
    "            data = json.loads(content)\n",
    "            return data[\"hallucination_type\"]\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            print(\"Retrying now...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       model_response  \\\n",
      "0   No, the FIFA World Cup will not be hosted this...   \n",
      "1   No, the FIFA World Cup will not be hosted this...   \n",
      "2   No, the FIFA World Cup will not be hosted this...   \n",
      "3   No, the FIFA World Cup will not be hosted this...   \n",
      "4   No, the FIFA World Cup will not be hosted this...   \n",
      "..                                                ...   \n",
      "95  No, the FIFA World Cup will not be hosted this...   \n",
      "96  No, the FIFA World Cup will not be hosted this...   \n",
      "97  No, the FIFA World Cup will not be hosted this...   \n",
      "98  No, the FIFA World Cup will not be held this y...   \n",
      "99  No, the FIFA World Cup will not be hosted this...   \n",
      "\n",
      "                                        needle  context_length  depth_percent  \\\n",
      "0   There won't be a FIFA World Cup this year.               0            0.0   \n",
      "1   There won't be a FIFA World Cup this year.               0           25.0   \n",
      "2   There won't be a FIFA World Cup this year.               0           50.0   \n",
      "3   There won't be a FIFA World Cup this year.               0           75.0   \n",
      "4   There won't be a FIFA World Cup this year.               0          100.0   \n",
      "..                                         ...             ...            ...   \n",
      "95  There won't be a FIFA World Cup this year.            5000            0.0   \n",
      "96  There won't be a FIFA World Cup this year.            5000           25.0   \n",
      "97  There won't be a FIFA World Cup this year.            5000           50.0   \n",
      "98  There won't be a FIFA World Cup this year.            5000           75.0   \n",
      "99  There won't be a FIFA World Cup this year.            5000          100.0   \n",
      "\n",
      "                                             question  frequency  \\\n",
      "0   Where will the FIFA World Cup be hosted this y...         10   \n",
      "1   Where will the FIFA World Cup be hosted this y...         10   \n",
      "2   Where will the FIFA World Cup be hosted this y...         10   \n",
      "3   Where will the FIFA World Cup be hosted this y...         10   \n",
      "4   Where will the FIFA World Cup be hosted this y...         10   \n",
      "..                                                ...        ...   \n",
      "95  Where will the FIFA World Cup be hosted this y...         18   \n",
      "96  Where will the FIFA World Cup be hosted this y...         18   \n",
      "97  Where will the FIFA World Cup be hosted this y...         18   \n",
      "98  Where will the FIFA World Cup be hosted this y...          1   \n",
      "99  Where will the FIFA World Cup be hosted this y...         18   \n",
      "\n",
      "                 category  \n",
      "0                relevant  \n",
      "1                relevant  \n",
      "2                relevant  \n",
      "3                relevant  \n",
      "4                relevant  \n",
      "..                    ...  \n",
      "95  irrelevant_misleading  \n",
      "96  irrelevant_misleading  \n",
      "97  irrelevant_misleading  \n",
      "98  irrelevant_misleading  \n",
      "99  irrelevant_misleading  \n",
      "\n",
      "[100 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open(f\"unique_responses/{id_val}.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "rows = []\n",
    "for category, entries in data.items():\n",
    "    for entry in entries:\n",
    "        entry[\"category\"] = category\n",
    "        rows.append(entry)\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Refining Q&A pairs:   5%|▌         | 5/100 [00:02<00:39,  2.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Rate limit pause: sleeping for 15 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Refining Q&A pairs:  10%|█         | 10/100 [00:19<02:17,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Rate limit pause: sleeping for 15 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Refining Q&A pairs:  15%|█▌        | 15/100 [00:36<02:21,  1.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Rate limit pause: sleeping for 15 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Refining Q&A pairs:  20%|██        | 20/100 [00:53<02:14,  1.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Rate limit pause: sleeping for 15 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Refining Q&A pairs:  25%|██▌       | 25/100 [01:10<02:07,  1.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Rate limit pause: sleeping for 15 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Refining Q&A pairs:  30%|███       | 30/100 [01:34<04:16,  3.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Rate limit pause: sleeping for 15 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Refining Q&A pairs:  35%|███▌      | 35/100 [02:01<04:49,  4.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Rate limit pause: sleeping for 15 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Refining Q&A pairs:  40%|████      | 40/100 [02:26<04:14,  4.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Rate limit pause: sleeping for 15 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Refining Q&A pairs:  45%|████▌     | 45/100 [02:51<03:55,  4.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Rate limit pause: sleeping for 15 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Refining Q&A pairs:  50%|█████     | 50/100 [03:16<03:36,  4.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Rate limit pause: sleeping for 15 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Refining Q&A pairs:  55%|█████▌    | 55/100 [03:41<03:14,  4.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Rate limit pause: sleeping for 15 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Refining Q&A pairs:  60%|██████    | 60/100 [04:07<02:52,  4.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Rate limit pause: sleeping for 15 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Refining Q&A pairs:  65%|██████▌   | 65/100 [04:32<02:30,  4.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Rate limit pause: sleeping for 15 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Refining Q&A pairs:  70%|███████   | 70/100 [04:56<01:59,  3.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Rate limit pause: sleeping for 15 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Refining Q&A pairs:  75%|███████▌  | 75/100 [05:22<01:51,  4.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Rate limit pause: sleeping for 15 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Refining Q&A pairs:  80%|████████  | 80/100 [05:47<01:26,  4.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Rate limit pause: sleeping for 15 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Refining Q&A pairs:  85%|████████▌ | 85/100 [06:12<01:04,  4.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Rate limit pause: sleeping for 15 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Refining Q&A pairs:  90%|█████████ | 90/100 [06:37<00:43,  4.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Rate limit pause: sleeping for 15 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Refining Q&A pairs:  95%|█████████▌| 95/100 [07:02<00:20,  4.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Rate limit pause: sleeping for 15 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Refining Q&A pairs: 100%|██████████| 100/100 [07:27<00:00,  4.48s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "hallucination_types = []\n",
    "\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Refining Q&A pairs\"):\n",
    "    mr, n, q = row.get(\"model_response\", \"\"), row.get(\"question_refined\", \"\"), row.get(\"needle_refined\", \"\")\n",
    "    # if s > 50:\n",
    "    #     current_hallucination_type = \"No Hallucination\"\n",
    "    \n",
    "    # else:\n",
    "    if idx > 0 and idx % 5 == 0:\n",
    "        print(\"⏳ Rate limit pause: sleeping for 15 seconds...\")\n",
    "        time.sleep(15)\n",
    "        \n",
    "    current_hallucination_type = rewrite_question_and_needle(mr, n, q)\n",
    "    hallucination_types.append(current_hallucination_type)\n",
    "\n",
    "# Update the DataFrame\n",
    "df['hallucination_type'] = hallucination_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by category and convert each group to a list of dicts\n",
    "json_output = {\n",
    "    category: group.drop(columns=\"category\").to_dict(orient=\"records\")\n",
    "    for category, group in df.groupby(\"category\")\n",
    "}\n",
    "\n",
    "output_dir = \"updated_unique_responses\"\n",
    "file_id = str(id_val)  # or whatever variable you're using\n",
    "output_path = os.path.join(output_dir, f\"{file_id}.json\")\n",
    "\n",
    "# Create the folder if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Write the JSON file into the folder\n",
    "with open(output_path, \"w\") as f:\n",
    "    json.dump(json_output, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
