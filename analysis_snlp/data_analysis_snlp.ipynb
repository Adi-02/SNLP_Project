{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview:\n",
    "\n",
    "This notebook processes and analyzes model test result JSON files to extract unique model responses, their scores and frequencies, and save the results for different context types. It includes functions to:\n",
    "\n",
    "- Load and process JSON files for a given context and ID.\n",
    "- Extract unique responses, their scores, and frequencies.\n",
    "- Save the processed data for all context types into a single JSON file for a specified ID.\n",
    "\n",
    "Additionally, it demonstrates saving the results for a specific ID as an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and Process JSON Files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import tqdm\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import time\n",
    "from groq import Groq\n",
    "\n",
    "\n",
    "\n",
    "def __get_unique_model_responses(id: int, context_type: str, with_misleading: bool = False) -> list:\n",
    "    \"\"\"\n",
    "    Load and process model test result JSON files for a given context and ID.\n",
    "\n",
    "    Returns a list of unique model responses with their:\n",
    "    - score (from first occurrence)\n",
    "    - frequency (how many times that response appeared)\n",
    "    \n",
    "    Sorted by descending score.\n",
    "    \"\"\"\n",
    "    model_name = 'llama-2-7b-80k'\n",
    "    base_dir = Path('../results/graph')\n",
    "    \n",
    "    suffix = '_misleading' if with_misleading else ''\n",
    "    directory = base_dir / f'{model_name}_id_{id}_{context_type}{suffix}'\n",
    "\n",
    "    # Collect all responses and scores\n",
    "    data = [\n",
    "        json.load(open(directory / file, encoding='utf-8'))\n",
    "        for file in os.listdir(directory) if file.endswith('.json')\n",
    "    ]\n",
    "    \n",
    "    df = pd.DataFrame(data)[['model_response', 'needle', 'context_length', 'depth_percent']]\n",
    "    question_df = pd.read_excel(\"../data_generation/FreshQADataset_with_misleading.xlsx\")\n",
    "    try:\n",
    "        question = question_df.set_index(\"id\").at[id, \"question\"]\n",
    "        df['question'] = question\n",
    "        \n",
    "    except KeyError:\n",
    "        raise ValueError(f\"No question found for id: {id}\")\n",
    "\n",
    "    # Compute frequency\n",
    "    freq = df['model_response'].value_counts().rename('frequency')\n",
    "\n",
    "    # Drop duplicates but keep first score\n",
    "    # df = df.drop_duplicates(subset='model_response')\n",
    "\n",
    "    # Merge with frequency\n",
    "    df = df.merge(freq, left_on='model_response', right_index=True)\n",
    "\n",
    "    # Sort by score descending\n",
    "    # df = df.sort_values(by='score', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    return df.to_dict(orient='records')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Processed Results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_all_contexts_for_id(id: int) -> None:\n",
    "    \"\"\"\n",
    "    Gather results from all 4 context types and save them in a single JSON file for the given id.\n",
    "\n",
    "    File will be saved as: unique_responses/{id}.json\n",
    "    \"\"\"\n",
    "    save_dir = Path('unique_responses')\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    all_data = {\n",
    "        'relevant': __get_unique_model_responses(id, 'relevant', with_misleading=False),\n",
    "        'relevant_misleading': __get_unique_model_responses(id, 'relevant', with_misleading=True),\n",
    "        'irrelevant': __get_unique_model_responses(id, 'irrelevant', with_misleading=False),\n",
    "        'irrelevant_misleading': __get_unique_model_responses(id, 'irrelevant', with_misleading=True),\n",
    "    }\n",
    "\n",
    "    save_path = save_dir / f'{id}.json'\n",
    "    with open(save_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(all_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"Saved all context responses for ID {id} to {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_val = 44"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved all context responses for ID 44 to unique_responses/44.json\n"
     ]
    }
   ],
   "source": [
    "save_all_contexts_for_id(id_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'model_response': 'No, the FIFA World Cup will not be hosted this year.',\n",
       "  'needle': \"There won't be a FIFA World Cup this year.\",\n",
       "  'context_length': 0,\n",
       "  'depth_percent': 0.0,\n",
       "  'question': 'Where will the FIFA World Cup be hosted this year?',\n",
       "  'frequency': 10},\n",
       " {'model_response': 'No, the FIFA World Cup will not be hosted this year.',\n",
       "  'needle': \"There won't be a FIFA World Cup this year.\",\n",
       "  'context_length': 0,\n",
       "  'depth_percent': 25.0,\n",
       "  'question': 'Where will the FIFA World Cup be hosted this year?',\n",
       "  'frequency': 10},\n",
       " {'model_response': 'No, the FIFA World Cup will not be hosted this year.',\n",
       "  'needle': \"There won't be a FIFA World Cup this year.\",\n",
       "  'context_length': 0,\n",
       "  'depth_percent': 50.0,\n",
       "  'question': 'Where will the FIFA World Cup be hosted this year?',\n",
       "  'frequency': 10},\n",
       " {'model_response': 'No, the FIFA World Cup will not be hosted this year.',\n",
       "  'needle': \"There won't be a FIFA World Cup this year.\",\n",
       "  'context_length': 0,\n",
       "  'depth_percent': 75.0,\n",
       "  'question': 'Where will the FIFA World Cup be hosted this year?',\n",
       "  'frequency': 10},\n",
       " {'model_response': 'No, the FIFA World Cup will not be hosted this year.',\n",
       "  'needle': \"There won't be a FIFA World Cup this year.\",\n",
       "  'context_length': 0,\n",
       "  'depth_percent': 100.0,\n",
       "  'question': 'Where will the FIFA World Cup be hosted this year?',\n",
       "  'frequency': 10},\n",
       " {'model_response': 'No, the FIFA World Cup will not be hosted this year. The FIFA World Cup is a major international football tournament that is held every four years. The tournament is organized by FIFA, the international governing body of football, and is contested by',\n",
       "  'needle': \"There won't be a FIFA World Cup this year.\",\n",
       "  'context_length': 1250,\n",
       "  'depth_percent': 0.0,\n",
       "  'question': 'Where will the FIFA World Cup be hosted this year?',\n",
       "  'frequency': 1},\n",
       " {'model_response': 'No, the FIFA World Cup will not be hosted this year.',\n",
       "  'needle': \"There won't be a FIFA World Cup this year.\",\n",
       "  'context_length': 1250,\n",
       "  'depth_percent': 25.0,\n",
       "  'question': 'Where will the FIFA World Cup be hosted this year?',\n",
       "  'frequency': 10},\n",
       " {'model_response': 'No, the FIFA World Cup will not be hosted this year.',\n",
       "  'needle': \"There won't be a FIFA World Cup this year.\",\n",
       "  'context_length': 1250,\n",
       "  'depth_percent': 50.0,\n",
       "  'question': 'Where will the FIFA World Cup be hosted this year?',\n",
       "  'frequency': 10},\n",
       " {'model_response': \"No, the FIFA World Cup will not be hosted this year. The FIFA World Cup is a major international football tournament that is held every four years. The tournament is organized by the Fédération Internationale de Football Association (FIFA), the sport'\",\n",
       "  'needle': \"There won't be a FIFA World Cup this year.\",\n",
       "  'context_length': 1250,\n",
       "  'depth_percent': 75.0,\n",
       "  'question': 'Where will the FIFA World Cup be hosted this year?',\n",
       "  'frequency': 1},\n",
       " {'model_response': 'No, the FIFA World Cup will not be hosted this year.',\n",
       "  'needle': \"There won't be a FIFA World Cup this year.\",\n",
       "  'context_length': 1250,\n",
       "  'depth_percent': 100.0,\n",
       "  'question': 'Where will the FIFA World Cup be hosted this year?',\n",
       "  'frequency': 10},\n",
       " {'model_response': 'No.',\n",
       "  'needle': \"There won't be a FIFA World Cup this year.\",\n",
       "  'context_length': 2500,\n",
       "  'depth_percent': 0.0,\n",
       "  'question': 'Where will the FIFA World Cup be hosted this year?',\n",
       "  'frequency': 10},\n",
       " {'model_response': 'No.',\n",
       "  'needle': \"There won't be a FIFA World Cup this year.\",\n",
       "  'context_length': 2500,\n",
       "  'depth_percent': 25.0,\n",
       "  'question': 'Where will the FIFA World Cup be hosted this year?',\n",
       "  'frequency': 10},\n",
       " {'model_response': 'No.',\n",
       "  'needle': \"There won't be a FIFA World Cup this year.\",\n",
       "  'context_length': 2500,\n",
       "  'depth_percent': 50.0,\n",
       "  'question': 'Where will the FIFA World Cup be hosted this year?',\n",
       "  'frequency': 10},\n",
       " {'model_response': 'No. The 1942 and 1946 competitions, which Germany and Brazil sought to host, were cancelled due to World War II.',\n",
       "  'needle': \"There won't be a FIFA World Cup this year.\",\n",
       "  'context_length': 2500,\n",
       "  'depth_percent': 75.0,\n",
       "  'question': 'Where will the FIFA World Cup be hosted this year?',\n",
       "  'frequency': 1},\n",
       " {'model_response': 'No, the FIFA World Cup will not be hosted this year.',\n",
       "  'needle': \"There won't be a FIFA World Cup this year.\",\n",
       "  'context_length': 2500,\n",
       "  'depth_percent': 100.0,\n",
       "  'question': 'Where will the FIFA World Cup be hosted this year?',\n",
       "  'frequency': 10},\n",
       " {'model_response': 'No.',\n",
       "  'needle': \"There won't be a FIFA World Cup this year.\",\n",
       "  'context_length': 3750,\n",
       "  'depth_percent': 0.0,\n",
       "  'question': 'Where will the FIFA World Cup be hosted this year?',\n",
       "  'frequency': 10},\n",
       " {'model_response': 'No.',\n",
       "  'needle': \"There won't be a FIFA World Cup this year.\",\n",
       "  'context_length': 3750,\n",
       "  'depth_percent': 25.0,\n",
       "  'question': 'Where will the FIFA World Cup be hosted this year?',\n",
       "  'frequency': 10},\n",
       " {'model_response': 'No.',\n",
       "  'needle': \"There won't be a FIFA World Cup this year.\",\n",
       "  'context_length': 3750,\n",
       "  'depth_percent': 50.0,\n",
       "  'question': 'Where will the FIFA World Cup be hosted this year?',\n",
       "  'frequency': 10},\n",
       " {'model_response': 'No.',\n",
       "  'needle': \"There won't be a FIFA World Cup this year.\",\n",
       "  'context_length': 3750,\n",
       "  'depth_percent': 75.0,\n",
       "  'question': 'Where will the FIFA World Cup be hosted this year?',\n",
       "  'frequency': 10},\n",
       " {'model_response': 'No. The FIFA World Cup is a quadrennial event. It is held every four years. The next FIFA World Cup will be held in 2022.',\n",
       "  'needle': \"There won't be a FIFA World Cup this year.\",\n",
       "  'context_length': 3750,\n",
       "  'depth_percent': 100.0,\n",
       "  'question': 'Where will the FIFA World Cup be hosted this year?',\n",
       "  'frequency': 1},\n",
       " {'model_response': 'No.',\n",
       "  'needle': \"There won't be a FIFA World Cup this year.\",\n",
       "  'context_length': 5000,\n",
       "  'depth_percent': 0.0,\n",
       "  'question': 'Where will the FIFA World Cup be hosted this year?',\n",
       "  'frequency': 10},\n",
       " {'model_response': 'No.',\n",
       "  'needle': \"There won't be a FIFA World Cup this year.\",\n",
       "  'context_length': 5000,\n",
       "  'depth_percent': 25.0,\n",
       "  'question': 'Where will the FIFA World Cup be hosted this year?',\n",
       "  'frequency': 10},\n",
       " {'model_response': 'No.',\n",
       "  'needle': \"There won't be a FIFA World Cup this year.\",\n",
       "  'context_length': 5000,\n",
       "  'depth_percent': 50.0,\n",
       "  'question': 'Where will the FIFA World Cup be hosted this year?',\n",
       "  'frequency': 10},\n",
       " {'model_response': 'No, the FIFA World Cup will not be hosted this year. The FIFA World Cup is a soccer tournament that is held every four years. The next FIFA World Cup will be held in 2022.',\n",
       "  'needle': \"There won't be a FIFA World Cup this year.\",\n",
       "  'context_length': 5000,\n",
       "  'depth_percent': 75.0,\n",
       "  'question': 'Where will the FIFA World Cup be hosted this year?',\n",
       "  'frequency': 1},\n",
       " {'model_response': 'No, the FIFA World Cup will not be hosted this year.',\n",
       "  'needle': \"There won't be a FIFA World Cup this year.\",\n",
       "  'context_length': 5000,\n",
       "  'depth_percent': 100.0,\n",
       "  'question': 'Where will the FIFA World Cup be hosted this year?',\n",
       "  'frequency': 10}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "__get_unique_model_responses(id_val, 'relevant', with_misleading=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rewrite_prompt(model_response, real_needle, question):\n",
    "    return (\n",
    "        f\"\"\"You are evaluating a model-generated response for hallucination analysis in a long-context retrieval setting.\n",
    "\n",
    "            You are given the following:\n",
    "\n",
    "            Question: {question}\n",
    "\n",
    "            The correct answer (real needle): {real_needle}\n",
    "\n",
    "            A model-generated response: {model_response}\n",
    "\n",
    "            Your task is to classify the model's response into one of the following categories:\n",
    "\n",
    "            1. Correct Retrieval: the model answered the question correctly\n",
    "\n",
    "            2. Excess Incorrect Retrieval: the model answered the question correctly but it also went on to output other text that was either completely irrelevant or invalid with regards to the question.\n",
    "\n",
    "            3. Irrelevant Retrieval: the model did not answer the question, it went on to output other text\n",
    "\n",
    "            4. Contradictory Retrieval: the model answered the question incorrectly\n",
    "\n",
    "            Examples:\n",
    "\n",
    "            Question: Who is the President of the United States? Needle: The President of the United States is Donald Trump.\n",
    "\n",
    "            1: The President of the United States is Donald Trump. (Correct Retrieval)\n",
    "\n",
    "            2: The President of the United States is Donald Trump. The affirmation was made in the Declaration of Independence, which was written predominantly by Thomas Jefferson in 1776. (Excess Incorrect Retrieval)\n",
    "\n",
    "            3: The President of the United States is the head of state and head of government of the United States. The president directs the executive branch of the federal government and is the commander-in-chief of the United States Armed Forces. (Irrelevant Retrieval)\n",
    "\n",
    "            4: Joe Biden is the President of the United States. (Contradictory Retrieval)\n",
    "\n",
    "            Return your output in this exact JSON format: {{\"model_response\": \"<text>\", \"needle\": \"<real_needle>\", \"hallucination_type\": \"<Correct Retrieval | Excess Incorrect Retrieval | Irrelevant Retrieval | Contradictory Retrieval>\"}}\"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load .env and API key\n",
    "load_dotenv()\n",
    "groq_api_key = os.getenv(\"snlp_api_key\")\n",
    "\n",
    "# Initialize Groq client\n",
    "client = Groq(api_key=groq_api_key)\n",
    "\n",
    "def rewrite_question_and_needle(response, needle, question):\n",
    "    while True:\n",
    "        try:\n",
    "            prompt = build_rewrite_prompt(response, needle, question)\n",
    "            completion = client.chat.completions.create(\n",
    "                model=\"llama-3.3-70b-versatile\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=0.7,\n",
    "                max_tokens=512,\n",
    "                top_p=1.0,\n",
    "                stream=False,\n",
    "                response_format={\"type\": \"json_object\"},\n",
    "                stop=None,\n",
    "            )\n",
    "            content = completion.choices[0].message.content\n",
    "            data = json.loads(content)\n",
    "            return data[\"hallucination_type\"]\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            print(\"Retrying now...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       model_response  \\\n",
      "0   No, the FIFA World Cup will not be hosted this...   \n",
      "1   No, the FIFA World Cup will not be hosted this...   \n",
      "2   No, the FIFA World Cup will not be hosted this...   \n",
      "3   No, the FIFA World Cup will not be hosted this...   \n",
      "4   No, the FIFA World Cup will not be hosted this...   \n",
      "..                                                ...   \n",
      "95  No, the FIFA World Cup will not be hosted this...   \n",
      "96  No, the FIFA World Cup will not be hosted this...   \n",
      "97  No, the FIFA World Cup will not be hosted this...   \n",
      "98  No, the FIFA World Cup will not be held this y...   \n",
      "99  No, the FIFA World Cup will not be hosted this...   \n",
      "\n",
      "                                        needle  context_length  depth_percent  \\\n",
      "0   There won't be a FIFA World Cup this year.               0            0.0   \n",
      "1   There won't be a FIFA World Cup this year.               0           25.0   \n",
      "2   There won't be a FIFA World Cup this year.               0           50.0   \n",
      "3   There won't be a FIFA World Cup this year.               0           75.0   \n",
      "4   There won't be a FIFA World Cup this year.               0          100.0   \n",
      "..                                         ...             ...            ...   \n",
      "95  There won't be a FIFA World Cup this year.            5000            0.0   \n",
      "96  There won't be a FIFA World Cup this year.            5000           25.0   \n",
      "97  There won't be a FIFA World Cup this year.            5000           50.0   \n",
      "98  There won't be a FIFA World Cup this year.            5000           75.0   \n",
      "99  There won't be a FIFA World Cup this year.            5000          100.0   \n",
      "\n",
      "                                             question  frequency  \\\n",
      "0   Where will the FIFA World Cup be hosted this y...         10   \n",
      "1   Where will the FIFA World Cup be hosted this y...         10   \n",
      "2   Where will the FIFA World Cup be hosted this y...         10   \n",
      "3   Where will the FIFA World Cup be hosted this y...         10   \n",
      "4   Where will the FIFA World Cup be hosted this y...         10   \n",
      "..                                                ...        ...   \n",
      "95  Where will the FIFA World Cup be hosted this y...         18   \n",
      "96  Where will the FIFA World Cup be hosted this y...         18   \n",
      "97  Where will the FIFA World Cup be hosted this y...         18   \n",
      "98  Where will the FIFA World Cup be hosted this y...          1   \n",
      "99  Where will the FIFA World Cup be hosted this y...         18   \n",
      "\n",
      "                 category  \n",
      "0                relevant  \n",
      "1                relevant  \n",
      "2                relevant  \n",
      "3                relevant  \n",
      "4                relevant  \n",
      "..                    ...  \n",
      "95  irrelevant_misleading  \n",
      "96  irrelevant_misleading  \n",
      "97  irrelevant_misleading  \n",
      "98  irrelevant_misleading  \n",
      "99  irrelevant_misleading  \n",
      "\n",
      "[100 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open(f\"unique_responses/{id_val}.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "rows = []\n",
    "for category, entries in data.items():\n",
    "    for entry in entries:\n",
    "        entry[\"category\"] = category\n",
    "        rows.append(entry)\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Refining Q&A pairs:   5%|▌         | 5/100 [00:02<00:39,  2.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Rate limit pause: sleeping for 15 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Refining Q&A pairs:  10%|█         | 10/100 [00:19<02:17,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Rate limit pause: sleeping for 15 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Refining Q&A pairs:  15%|█▌        | 15/100 [00:36<02:21,  1.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Rate limit pause: sleeping for 15 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Refining Q&A pairs:  20%|██        | 20/100 [00:53<02:14,  1.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Rate limit pause: sleeping for 15 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Refining Q&A pairs:  25%|██▌       | 25/100 [01:10<02:07,  1.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Rate limit pause: sleeping for 15 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Refining Q&A pairs:  30%|███       | 30/100 [01:34<04:16,  3.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Rate limit pause: sleeping for 15 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Refining Q&A pairs:  35%|███▌      | 35/100 [02:01<04:49,  4.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Rate limit pause: sleeping for 15 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Refining Q&A pairs:  40%|████      | 40/100 [02:26<04:14,  4.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Rate limit pause: sleeping for 15 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Refining Q&A pairs:  45%|████▌     | 45/100 [02:51<03:55,  4.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Rate limit pause: sleeping for 15 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Refining Q&A pairs:  50%|█████     | 50/100 [03:16<03:36,  4.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Rate limit pause: sleeping for 15 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Refining Q&A pairs:  55%|█████▌    | 55/100 [03:41<03:14,  4.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Rate limit pause: sleeping for 15 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Refining Q&A pairs:  60%|██████    | 60/100 [04:07<02:52,  4.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Rate limit pause: sleeping for 15 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Refining Q&A pairs:  65%|██████▌   | 65/100 [04:32<02:30,  4.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Rate limit pause: sleeping for 15 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Refining Q&A pairs:  70%|███████   | 70/100 [04:56<01:59,  3.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Rate limit pause: sleeping for 15 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Refining Q&A pairs:  75%|███████▌  | 75/100 [05:22<01:51,  4.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Rate limit pause: sleeping for 15 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Refining Q&A pairs:  80%|████████  | 80/100 [05:47<01:26,  4.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Rate limit pause: sleeping for 15 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Refining Q&A pairs:  85%|████████▌ | 85/100 [06:12<01:04,  4.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Rate limit pause: sleeping for 15 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Refining Q&A pairs:  90%|█████████ | 90/100 [06:37<00:43,  4.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Rate limit pause: sleeping for 15 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Refining Q&A pairs:  95%|█████████▌| 95/100 [07:02<00:20,  4.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Rate limit pause: sleeping for 15 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Refining Q&A pairs: 100%|██████████| 100/100 [07:27<00:00,  4.48s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "hallucination_types = []\n",
    "\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Refining Q&A pairs\"):\n",
    "    mr, n, q = row.get(\"model_response\", \"\"), row.get(\"needle\", \"\"), row.get(\"question\", \"\")\n",
    "    # if s > 50:\n",
    "    #     current_hallucination_type = \"No Hallucination\"\n",
    "    \n",
    "    # else:\n",
    "    if idx > 0 and idx % 5 == 0:\n",
    "        print(\"⏳ Rate limit pause: sleeping for 15 seconds...\")\n",
    "        time.sleep(15)\n",
    "        \n",
    "    current_hallucination_type = rewrite_question_and_needle(mr, n, q)\n",
    "    hallucination_types.append(current_hallucination_type)\n",
    "\n",
    "# Update the DataFrame\n",
    "df['hallucination_type'] = hallucination_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by category and convert each group to a list of dicts\n",
    "json_output = {\n",
    "    category: group.drop(columns=\"category\").to_dict(orient=\"records\")\n",
    "    for category, group in df.groupby(\"category\")\n",
    "}\n",
    "\n",
    "output_dir = \"updated_unique_responses\"\n",
    "file_id = str(id_val)  # or whatever variable you're using\n",
    "output_path = os.path.join(output_dir, f\"{file_id}.json\")\n",
    "\n",
    "# Create the folder if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Write the JSON file into the folder\n",
    "with open(output_path, \"w\") as f:\n",
    "    json.dump(json_output, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
