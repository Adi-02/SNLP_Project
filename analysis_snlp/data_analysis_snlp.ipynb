{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview:\n",
    "\n",
    "This notebook processes and analyzes model test result JSON files to extract unique model responses, their scores and frequencies, and save the results for different context types. It includes functions to:\n",
    "\n",
    "- Load and process JSON files for a given context and ID.\n",
    "- Extract unique responses, their scores, and frequencies.\n",
    "- Save the processed data for all context types into a single JSON file for a specified ID.\n",
    "\n",
    "Additionally, it demonstrates saving the results for a specific ID as an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and Process JSON Files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "def __get_unique_model_responses(id: int, context_type: str, with_misleading: bool = False, att_heads_mask: bool = False) -> list:\n",
    "    \"\"\"\n",
    "    Load and process model test result JSON files for a given context and ID.\n",
    "\n",
    "    Returns a list of unique model responses with their:\n",
    "    - score (from first occurrence)\n",
    "    - frequency (how many times that response appeared)\n",
    "    \n",
    "    Sorted by descending score.\n",
    "    \"\"\"\n",
    "    model_name = 'llama-2-7b-80k'\n",
    "    base_dir = Path('../results/graph')\n",
    "    \n",
    "    suffix = '_misleading' if with_misleading else ''\n",
    "    directory = base_dir / f'{model_name}_id_{id}_{context_type}{suffix}'\n",
    "    if att_heads_mask:\n",
    "        directory = directory.parent / (directory.name + \"_block_top30\")\n",
    "\n",
    "    # Collect all responses and scores\n",
    "    data = [\n",
    "        json.load(open(directory / file, encoding='utf-8'))\n",
    "        for file in os.listdir(directory) if file.endswith('.json')\n",
    "    ]\n",
    "    \n",
    "    df = pd.DataFrame(data)[['model_response', 'needle', 'context_length', 'depth_percent']]\n",
    "    question_df = pd.read_json(\"../data_generation/context_cleaned.json\")\n",
    "    try:\n",
    "        question = question_df.set_index(\"id\").at[id, \"question_refined\"]\n",
    "        df['question'] = question\n",
    "        \n",
    "    except KeyError:\n",
    "        raise ValueError(f\"No question found for id: {id}\")\n",
    "\n",
    "    # Compute frequency\n",
    "    freq = df['model_response'].value_counts().rename('frequency')\n",
    "\n",
    "    # Merge with frequency\n",
    "    df = df.merge(freq, left_on='model_response', right_index=True)\n",
    "\n",
    "    return df.to_dict(orient='records')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Processed Results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_all_contexts_for_id(id: int, attn_mask_block_30: bool = False) -> None:\n",
    "    \"\"\"\n",
    "    Gather results from all 4 context types and save them in a single JSON file for the given id.\n",
    "\n",
    "    File will be saved as: unique_responses/{id}.json\n",
    "    \"\"\"\n",
    "    save_dir = Path('unique_responses')\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    if attn_mask_block_30:\n",
    "        block_save_dir = save_dir.parent / (save_dir.name + '_block_top30')\n",
    "        block_save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    all_data = {\n",
    "        'relevant': __get_unique_model_responses(id, 'relevant', with_misleading=False, att_heads_mask=attn_mask_block_30),\n",
    "        'relevant_misleading': __get_unique_model_responses(id, 'relevant', with_misleading=True, att_heads_mask=attn_mask_block_30),\n",
    "        'irrelevant': __get_unique_model_responses(id, 'irrelevant', with_misleading=False, att_heads_mask=attn_mask_block_30),\n",
    "        'irrelevant_misleading': __get_unique_model_responses(id, 'irrelevant', with_misleading=True, att_heads_mask=attn_mask_block_30),\n",
    "    }\n",
    "\n",
    "    save_path = save_dir / f'{id}.json'\n",
    "    if attn_mask_block_30:\n",
    "        save_path = block_save_dir / f'{id}.json'\n",
    "    with open(save_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(all_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"Saved all context responses for ID {id} to {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 11, 121, 122, 123, 124, 14, 15, 155, 16, 160, 163, 164, 167, 168, 17, 170, 172, 173, 175, 18, 180, 182, 183, 185, 189, 192, 193, 196, 198, 2, 29, 3, 380, 391, 393, 396, 4, 401, 403, 408, 409, 410, 411, 420, 421, 422, 423, 424, 427, 43, 430, 432, 433, 435, 436, 437, 438, 439, 44, 441, 442, 444, 448, 449, 453, 5, 532, 535, 539, 576, 577, 578, 579, 586, 587, 588, 589, 590, 92, 95, 96]\n"
     ]
    }
   ],
   "source": [
    "# id_val = 44\n",
    "ids = []\n",
    "for file_name in os.listdir('../haystack/irrelevant'):\n",
    "    if file_name.endswith('.txt'):\n",
    "        file_id = os.path.splitext(file_name)[0]  # Extract the file name without extension\n",
    "        ids.append(int(file_id))\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved all context responses for ID 1 to unique_responses_block_top30\\1.json\n",
      "Saved all context responses for ID 11 to unique_responses_block_top30\\11.json\n",
      "Saved all context responses for ID 121 to unique_responses_block_top30\\121.json\n",
      "Saved all context responses for ID 122 to unique_responses_block_top30\\122.json\n",
      "Saved all context responses for ID 123 to unique_responses_block_top30\\123.json\n",
      "Saved all context responses for ID 124 to unique_responses_block_top30\\124.json\n",
      "Saved all context responses for ID 14 to unique_responses_block_top30\\14.json\n",
      "Saved all context responses for ID 15 to unique_responses_block_top30\\15.json\n",
      "Saved all context responses for ID 155 to unique_responses_block_top30\\155.json\n",
      "Saved all context responses for ID 16 to unique_responses_block_top30\\16.json\n",
      "Saved all context responses for ID 160 to unique_responses_block_top30\\160.json\n",
      "Saved all context responses for ID 163 to unique_responses_block_top30\\163.json\n",
      "Saved all context responses for ID 164 to unique_responses_block_top30\\164.json\n",
      "Saved all context responses for ID 167 to unique_responses_block_top30\\167.json\n",
      "Saved all context responses for ID 168 to unique_responses_block_top30\\168.json\n",
      "Saved all context responses for ID 17 to unique_responses_block_top30\\17.json\n",
      "Saved all context responses for ID 170 to unique_responses_block_top30\\170.json\n",
      "Saved all context responses for ID 172 to unique_responses_block_top30\\172.json\n",
      "Saved all context responses for ID 173 to unique_responses_block_top30\\173.json\n",
      "Saved all context responses for ID 175 to unique_responses_block_top30\\175.json\n",
      "Saved all context responses for ID 18 to unique_responses_block_top30\\18.json\n",
      "Saved all context responses for ID 180 to unique_responses_block_top30\\180.json\n",
      "Saved all context responses for ID 182 to unique_responses_block_top30\\182.json\n",
      "Saved all context responses for ID 183 to unique_responses_block_top30\\183.json\n",
      "Saved all context responses for ID 185 to unique_responses_block_top30\\185.json\n",
      "Saved all context responses for ID 189 to unique_responses_block_top30\\189.json\n",
      "Saved all context responses for ID 192 to unique_responses_block_top30\\192.json\n",
      "Saved all context responses for ID 193 to unique_responses_block_top30\\193.json\n",
      "Saved all context responses for ID 196 to unique_responses_block_top30\\196.json\n",
      "Saved all context responses for ID 198 to unique_responses_block_top30\\198.json\n",
      "Saved all context responses for ID 2 to unique_responses_block_top30\\2.json\n",
      "Saved all context responses for ID 29 to unique_responses_block_top30\\29.json\n",
      "Saved all context responses for ID 3 to unique_responses_block_top30\\3.json\n",
      "Saved all context responses for ID 380 to unique_responses_block_top30\\380.json\n",
      "Saved all context responses for ID 391 to unique_responses_block_top30\\391.json\n",
      "Saved all context responses for ID 393 to unique_responses_block_top30\\393.json\n",
      "Saved all context responses for ID 396 to unique_responses_block_top30\\396.json\n",
      "Saved all context responses for ID 4 to unique_responses_block_top30\\4.json\n",
      "Saved all context responses for ID 401 to unique_responses_block_top30\\401.json\n",
      "Saved all context responses for ID 403 to unique_responses_block_top30\\403.json\n",
      "Saved all context responses for ID 408 to unique_responses_block_top30\\408.json\n",
      "Saved all context responses for ID 409 to unique_responses_block_top30\\409.json\n",
      "Saved all context responses for ID 410 to unique_responses_block_top30\\410.json\n",
      "Saved all context responses for ID 411 to unique_responses_block_top30\\411.json\n",
      "Saved all context responses for ID 420 to unique_responses_block_top30\\420.json\n",
      "Saved all context responses for ID 421 to unique_responses_block_top30\\421.json\n",
      "Saved all context responses for ID 422 to unique_responses_block_top30\\422.json\n",
      "Saved all context responses for ID 423 to unique_responses_block_top30\\423.json\n",
      "Saved all context responses for ID 424 to unique_responses_block_top30\\424.json\n",
      "Saved all context responses for ID 427 to unique_responses_block_top30\\427.json\n",
      "Saved all context responses for ID 43 to unique_responses_block_top30\\43.json\n",
      "Saved all context responses for ID 430 to unique_responses_block_top30\\430.json\n",
      "Saved all context responses for ID 432 to unique_responses_block_top30\\432.json\n",
      "Saved all context responses for ID 433 to unique_responses_block_top30\\433.json\n",
      "Saved all context responses for ID 435 to unique_responses_block_top30\\435.json\n",
      "Saved all context responses for ID 436 to unique_responses_block_top30\\436.json\n",
      "Saved all context responses for ID 437 to unique_responses_block_top30\\437.json\n",
      "Saved all context responses for ID 438 to unique_responses_block_top30\\438.json\n",
      "Saved all context responses for ID 439 to unique_responses_block_top30\\439.json\n",
      "Saved all context responses for ID 44 to unique_responses_block_top30\\44.json\n",
      "Saved all context responses for ID 441 to unique_responses_block_top30\\441.json\n",
      "Saved all context responses for ID 442 to unique_responses_block_top30\\442.json\n",
      "Saved all context responses for ID 444 to unique_responses_block_top30\\444.json\n",
      "Saved all context responses for ID 448 to unique_responses_block_top30\\448.json\n",
      "Saved all context responses for ID 449 to unique_responses_block_top30\\449.json\n",
      "Saved all context responses for ID 453 to unique_responses_block_top30\\453.json\n",
      "Saved all context responses for ID 5 to unique_responses_block_top30\\5.json\n",
      "Saved all context responses for ID 532 to unique_responses_block_top30\\532.json\n",
      "Saved all context responses for ID 535 to unique_responses_block_top30\\535.json\n",
      "Saved all context responses for ID 539 to unique_responses_block_top30\\539.json\n",
      "Saved all context responses for ID 576 to unique_responses_block_top30\\576.json\n",
      "Saved all context responses for ID 577 to unique_responses_block_top30\\577.json\n",
      "Saved all context responses for ID 578 to unique_responses_block_top30\\578.json\n",
      "Saved all context responses for ID 579 to unique_responses_block_top30\\579.json\n",
      "Saved all context responses for ID 586 to unique_responses_block_top30\\586.json\n",
      "Saved all context responses for ID 587 to unique_responses_block_top30\\587.json\n",
      "Saved all context responses for ID 588 to unique_responses_block_top30\\588.json\n",
      "Saved all context responses for ID 589 to unique_responses_block_top30\\589.json\n",
      "Saved all context responses for ID 590 to unique_responses_block_top30\\590.json\n",
      "Saved all context responses for ID 92 to unique_responses_block_top30\\92.json\n",
      "Saved all context responses for ID 95 to unique_responses_block_top30\\95.json\n",
      "Saved all context responses for ID 96 to unique_responses_block_top30\\96.json\n"
     ]
    }
   ],
   "source": [
    "# Change to true for masking top 30 attention heads\n",
    "attn_mask_block = True\n",
    "for i in ids:\n",
    "    save_all_contexts_for_id(i, attn_mask_block_30=attn_mask_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rewrite_prompt(model_response, real_needle, question):\n",
    "    return (\n",
    "        f\"\"\"You are evaluating a model-generated response for hallucination analysis in a long-context retrieval setting.\n",
    "\n",
    "            You are given the following:\n",
    "\n",
    "            Question: {question}\n",
    "\n",
    "            The correct answer (real needle): {real_needle}\n",
    "\n",
    "            A model-generated response: {model_response}\n",
    "\n",
    "            Your task is to classify the model's response into one of the following categories:\n",
    "            1. Correct Retrieval: The model_response is an answer that is semantically aligned with the real_needle,  providing a factually accurate answer with no additional information that is both incorrect and unrelated.\n",
    "            2. Excess Incorrect Retrieval: Part of the model_response is semantically aligned with the real_needle, but it also contains additional content that is either factually incorrect or unrelated to the original question.\n",
    "            3. Irrelevant Retrieval: The model_response is not semantically aligned with the real_needle. Instead, it provides content that is unrelated or loosely associated with the question, without answering it directly.\n",
    "            4. Contradictory Retrieval: The model_response is an incorrect answer that directly contradicts the real_needle or a factually false statement in place of the correct response.\n",
    "\n",
    "            Example 1:\n",
    "\n",
    "            Question: When was the Republic of Artsakh formally dissolved?\n",
    "            Needle: On 1 January 2024, the Republic of Artsakh was formally dissolved.\n",
    "\n",
    "            Example Correct Retrieval:\n",
    "            - On 1 January 2024.\n",
    "            - On 1 January 2024, the Republic of Artsakh was dissolved.\n",
    "            - On 1 January 2024, the Republic of Artsakh was formally dissolved.\n",
    "\n",
    "            Example Excess Retrieval:\n",
    "            - On 1 January 2024, the Republic of Artsakh was formally dissolved. Following Qajar Iran's loss, it was forced to concede suzerainty over most of the khanates, along with Georgia.\n",
    "            - On 1 January 2024, the Republic of Artsakh was formally dissolved. The original etymology of this name is thought to have its roots in the once-dominant Zoroastrianism. In the Avesta\n",
    "            - On 1 January 2024, the Republic of Artsakh was formally dissolved. Russia actively tried to gain possession of the Caucasus region which was, for the most part, in the hands of Iran. In\n",
    "            - On 1 January 2024, the Republic of Artsakh was formally dissolved. The region is usually equated with the administrative borders of the former Nagorno-Karabakh Autonomous Oblast, comprising 4,\n",
    "\n",
    "            Example Irrelevant Retrieval:\n",
    "            - On 27 September 2020, the Second Nagorno-Karabakh War broke out with an Azerbaijani offensive in Nagorno-Karabakh and the surrounding territories. Azerbaijan made\n",
    "\n",
    "            Example Contradictory Retrieval:\n",
    "            - The Republic of Artsakh was formally dissolved on September 27, 2020.\n",
    "            - On September 27, 2020, the Republic of Artsakh was formally dissolved.\n",
    "            - The Republic of Artsakh was formally dissolved on September 27, 2023.\n",
    "            The Republic of Artsakh was formally dissolved on 27 September 2020, after the Second Nagorno-Karabakh War.\n",
    "\n",
    "            Example 2:\n",
    "\n",
    "            Question: Where will the FIFA World Cup be hosted this year?\n",
    "            Needle: There won't be a FIFA World Cup this year.\n",
    "\n",
    "            Example Correct Retrieval:\n",
    "            - No, the FIFA World Cup will not be hosted this year.\n",
    "            - No.\n",
    "\n",
    "            Example Excess Retrieval: \n",
    "            - No, the FIFA World Cup will not be held this year. The FIFA World Cup is a quadrennial international football tournament contested by the senior men's national teams of the member associations of FIFA, the sport's global governing\n",
    "\n",
    "            Example Irrelevant Retrieval:\n",
    "            - The FIFA World Cup, often called the World Cup, is an international association football competition among the senior men's national teams of the members of the Fédération Internationale de Football Association (FIFA), the sport's global governing body.\n",
    "\n",
    "            Example Contradictory:\n",
    "            - No. The FIFA World Cup is a soccer tournament that is held every four years. The next World Cup will be held in 2022.\n",
    "\n",
    "            Return your output in this exact JSON format: {{\"model_response\": \"<text>\", \"needle\": \"<real_needle>\", \"hallucination_type\": \"<Correct Retrieval | Excess Incorrect Retrieval | Irrelevant Retrieval | Contradictory Retrieval>\"}}\"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load .env and API key\n",
    "load_dotenv()\n",
    "together_api_key = os.getenv(\"together_api_key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from together import Together\n",
    "\n",
    "client = Together(api_key=together_api_key)\n",
    "\n",
    "def hallucination_categorising(response, needle, question):\n",
    "    while True:\n",
    "        try:\n",
    "            prompt = build_rewrite_prompt(response, needle, question)\n",
    "            completion = client.chat.completions.create(\n",
    "                model=\"meta-llama/Llama-3.3-70B-Instruct-Turbo\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=0.7,\n",
    "                max_tokens=512,\n",
    "                top_p=1.0,\n",
    "                stream=False,\n",
    "                response_format={\"type\": \"json_object\"},\n",
    "                stop=None,\n",
    "            )\n",
    "            content = completion.choices[0].message.content\n",
    "            data = json.loads(content)\n",
    "            return data[\"hallucination_type\"]\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            print(\"Retrying now...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_dfs = []\n",
    "responses_path = \"unique_responses\" if not attn_mask_block else \"unique_responses_block_top30\"\n",
    "for i in ids:\n",
    "    with open(f\"{responses_path}/{i}.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "        \n",
    "    rows = []\n",
    "    for category, entries in data.items():\n",
    "        for entry in entries:\n",
    "            entry[\"category\"] = category\n",
    "            rows.append(entry)\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    lst_dfs.append(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82\n"
     ]
    }
   ],
   "source": [
    "print(len(lst_dfs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Categorising hallucinations: 100%|██████████| 100/100 [02:57<00:00,  1.78s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [02:56<00:00,  1.76s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [03:03<00:00,  1.84s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [02:43<00:00,  1.64s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [03:08<00:00,  1.88s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [02:40<00:00,  1.61s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [02:41<00:00,  1.62s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [02:45<00:00,  1.66s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [03:14<00:00,  1.94s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [02:36<00:00,  1.56s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [02:28<00:00,  1.48s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [02:59<00:00,  1.79s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [02:45<00:00,  1.65s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [02:49<00:00,  1.70s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [03:06<00:00,  1.86s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [02:46<00:00,  1.67s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [02:31<00:00,  1.52s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [02:17<00:00,  1.37s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [02:19<00:00,  1.40s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [02:46<00:00,  1.66s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [03:03<00:00,  1.83s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [02:26<00:00,  1.47s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [02:36<00:00,  1.57s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [02:34<00:00,  1.54s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [02:42<00:00,  1.62s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [03:05<00:00,  1.86s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [02:40<00:00,  1.60s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [02:31<00:00,  1.52s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [02:09<00:00,  1.29s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [02:32<00:00,  1.53s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [02:51<00:00,  1.72s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [02:15<00:00,  1.36s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [03:18<00:00,  1.98s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [03:06<00:00,  1.86s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [02:59<00:00,  1.80s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [04:23<00:00,  2.63s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [02:21<00:00,  1.42s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [02:45<00:00,  1.66s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [02:47<00:00,  1.68s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [02:23<00:00,  1.44s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [02:54<00:00,  1.75s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [02:35<00:00,  1.55s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [03:22<00:00,  2.02s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [02:28<00:00,  1.48s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [02:18<00:00,  1.39s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [02:42<00:00,  1.63s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [02:31<00:00,  1.52s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [06:36<00:00,  3.97s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [03:09<00:00,  1.89s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [03:23<00:00,  2.03s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [02:38<00:00,  1.58s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [02:21<00:00,  1.42s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [02:13<00:00,  1.33s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [02:07<00:00,  1.28s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [02:15<00:00,  1.35s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [02:40<00:00,  1.61s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [02:24<00:00,  1.44s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [02:33<00:00,  1.53s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [02:25<00:00,  1.45s/it]\n",
      "Categorising hallucinations:   8%|▊         | 8/100 [00:11<02:15,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Expecting ',' delimiter: line 1186 column 1 (char 2117)\n",
      "Retrying now...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Categorising hallucinations: 100%|██████████| 100/100 [02:33<00:00,  1.54s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [02:53<00:00,  1.73s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [02:01<00:00,  1.21s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [01:51<00:00,  1.11s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [02:09<00:00,  1.29s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [02:22<00:00,  1.42s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [16:44<00:00, 10.04s/it] \n",
      "Categorising hallucinations: 100%|██████████| 100/100 [26:54<00:00, 16.15s/it]  \n",
      "Categorising hallucinations: 100%|██████████| 100/100 [05:47<00:00,  3.47s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [03:01<00:00,  1.81s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [02:56<00:00,  1.76s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [02:49<00:00,  1.70s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [06:20<00:00,  3.80s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [02:54<00:00,  1.74s/it]\n",
      "Categorising hallucinations:  92%|█████████▏| 92/100 [02:48<00:09,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Expecting value: line 1 column 19 (char 18)\n",
      "Retrying now...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Categorising hallucinations: 100%|██████████| 100/100 [03:02<00:00,  1.82s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [02:45<00:00,  1.65s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [02:28<00:00,  1.49s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [02:18<00:00,  1.38s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [02:11<00:00,  1.31s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [02:44<00:00,  1.64s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [02:59<00:00,  1.80s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [02:39<00:00,  1.60s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [02:34<00:00,  1.54s/it]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "for single_df in lst_dfs:\n",
    "    hallucination_types = []\n",
    "\n",
    "    for idx, row in tqdm(single_df.iterrows(), total=len(single_df), desc=\"Categorising hallucinations\"):\n",
    "        start_time = time.time()  # ⏱ Track time at the start of each request\n",
    "\n",
    "        mr = row.get(\"model_response\", \"\")\n",
    "        n = row.get(\"needle\", \"\")\n",
    "        q = row.get(\"question\", \"\")\n",
    "\n",
    "        current_hallucination_type = hallucination_categorising(mr, n, q)\n",
    "        hallucination_types.append(current_hallucination_type)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "        sleep_time = max(0, 1.0 - elapsed)  # ⏸ Sleep the remaining time to enforce 1 req/sec\n",
    "        time.sleep(sleep_time)\n",
    "\n",
    "    single_df['hallucination_type'] = hallucination_types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df_single, id_single in zip(lst_dfs, ids):\n",
    "    # Group by category and convert each group to a list of dicts\n",
    "    json_output = {\n",
    "        category: group.drop(columns=\"category\").to_dict(orient=\"records\")\n",
    "        for category, group in df_single.groupby(\"category\")\n",
    "    }\n",
    "\n",
    "    output_dir = \"updated_unique_responses\" if not attn_mask_block else \"updated_unique_responses_block_top30\"\n",
    "    file_id = str(id_single)  # or whatever variable you're using\n",
    "    output_path = os.path.join(output_dir, f\"{file_id}.json\")\n",
    "\n",
    "    # Create the folder if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Write the JSON file into the folder\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(json_output, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
