{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview:\n",
    "\n",
    "This notebook processes and analyzes model test result JSON files to extract unique model responses, their scores and frequencies, and save the results for different context types. It includes functions to:\n",
    "\n",
    "- Load and process JSON files for a given context and ID.\n",
    "- Extract unique responses, their scores, and frequencies.\n",
    "- Save the processed data for all context types into a single JSON file for a specified ID.\n",
    "\n",
    "Additionally, it demonstrates saving the results for a specific ID as an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and Process JSON Files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import tqdm\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import time\n",
    "from groq import Groq\n",
    "\n",
    "\n",
    "\n",
    "def __get_unique_model_responses(id: int, context_type: str, with_misleading: bool = False) -> list:\n",
    "    \"\"\"\n",
    "    Load and process model test result JSON files for a given context and ID.\n",
    "\n",
    "    Returns a list of unique model responses with their:\n",
    "    - score (from first occurrence)\n",
    "    - frequency (how many times that response appeared)\n",
    "    \n",
    "    Sorted by descending score.\n",
    "    \"\"\"\n",
    "    model_name = 'llama-2-7b-80k'\n",
    "    base_dir = Path('../results/graph')\n",
    "    \n",
    "    suffix = '_misleading' if with_misleading else ''\n",
    "    directory = base_dir / f'{model_name}_id_{id}_{context_type}{suffix}'\n",
    "\n",
    "    # Collect all responses and scores\n",
    "    data = [\n",
    "        json.load(open(directory / file, encoding='utf-8'))\n",
    "        for file in os.listdir(directory) if file.endswith('.json')\n",
    "    ]\n",
    "    \n",
    "    df = pd.DataFrame(data)[['model_response', 'score', 'needle']]\n",
    "\n",
    "    # Compute frequency\n",
    "    freq = df['model_response'].value_counts().rename('frequency')\n",
    "\n",
    "    # Drop duplicates but keep first score\n",
    "    df = df.drop_duplicates(subset='model_response')\n",
    "\n",
    "    # Merge with frequency\n",
    "    df = df.merge(freq, left_on='model_response', right_index=True)\n",
    "\n",
    "    # Sort by score descending\n",
    "    df = df.sort_values(by='score', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    return df.to_dict(orient='records')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Processed Results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_all_contexts_for_id(id: int) -> None:\n",
    "    \"\"\"\n",
    "    Gather results from all 4 context types and save them in a single JSON file for the given id.\n",
    "\n",
    "    File will be saved as: unique_responses/{id}.json\n",
    "    \"\"\"\n",
    "    save_dir = Path('unique_responses')\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    all_data = {\n",
    "        'relevant': __get_unique_model_responses(id, 'relevant', with_misleading=False),\n",
    "        'relevant_misleading': __get_unique_model_responses(id, 'relevant', with_misleading=True),\n",
    "        'irrelevant': __get_unique_model_responses(id, 'irrelevant', with_misleading=False),\n",
    "        'irrelevant_misleading': __get_unique_model_responses(id, 'irrelevant', with_misleading=True),\n",
    "    }\n",
    "\n",
    "    save_path = save_dir / f'{id}.json'\n",
    "    with open(save_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(all_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"Saved all context responses for ID {id} to {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_val = 44"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved all context responses for ID 44 to unique_responses/44.json\n"
     ]
    }
   ],
   "source": [
    "save_all_contexts_for_id(44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'model_response': 'No, the FIFA World Cup will not be hosted this year.',\n",
       "  'score': 68.16167831420898,\n",
       "  'needle': \"There won't be a FIFA World Cup this year.\",\n",
       "  'frequency': 10},\n",
       " {'model_response': 'No, the FIFA World Cup will not be hosted this year. The FIFA World Cup is a major international football tournament that is held every four years. The tournament is organized by FIFA, the international governing body of football, and is contested by',\n",
       "  'score': 64.01805877685547,\n",
       "  'needle': \"There won't be a FIFA World Cup this year.\",\n",
       "  'frequency': 1},\n",
       " {'model_response': \"No, the FIFA World Cup will not be hosted this year. The FIFA World Cup is a major international football tournament that is held every four years. The tournament is organized by the Fédération Internationale de Football Association (FIFA), the sport'\",\n",
       "  'score': 63.443970680236816,\n",
       "  'needle': \"There won't be a FIFA World Cup this year.\",\n",
       "  'frequency': 1},\n",
       " {'model_response': 'No, the FIFA World Cup will not be hosted this year. The FIFA World Cup is a soccer tournament that is held every four years. The next FIFA World Cup will be held in 2022.',\n",
       "  'score': 61.14392280578613,\n",
       "  'needle': \"There won't be a FIFA World Cup this year.\",\n",
       "  'frequency': 1},\n",
       " {'model_response': 'No. The FIFA World Cup is a quadrennial event. It is held every four years. The next FIFA World Cup will be held in 2022.',\n",
       "  'score': 52.48445272445679,\n",
       "  'needle': \"There won't be a FIFA World Cup this year.\",\n",
       "  'frequency': 1},\n",
       " {'model_response': 'No. The 1942 and 1946 competitions, which Germany and Brazil sought to host, were cancelled due to World War II.',\n",
       "  'score': 42.26422905921936,\n",
       "  'needle': \"There won't be a FIFA World Cup this year.\",\n",
       "  'frequency': 1},\n",
       " {'model_response': 'No.',\n",
       "  'score': 11.441656947135925,\n",
       "  'needle': \"There won't be a FIFA World Cup this year.\",\n",
       "  'frequency': 10}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "__get_unique_model_responses(id_val, 'relevant', with_misleading=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rewrite_prompt(model_response, real_needle):\n",
    "    return (\n",
    "        f\"You are evaluating a model-generated response for hallucination analysis.\\n\"\n",
    "        f\"You are given:\\n\"\n",
    "        f\"- The correct answer (real needle)\\n\"\n",
    "        f\"- A model's generated response\\n\\n\"\n",
    "        f\"Your task is to categorize the model's response as one of the following hallucination types:\\n\"\n",
    "        f\"1. **Selective or Incomplete Retrieval**: The response is factually correct but misses key information needed to fully match the real needle.\\n\"\n",
    "        f\"2. **Contradictory Hallucination**: The response contains incorrect or conflicting information compared to the real needle.\\n\\n\"\n",
    "        f\"Use the real needle to judge what information is missing or contradicted.\\n\\n\"\n",
    "        f\"Examples:\\n\"\n",
    "        f\"- If the real needle is 'Joe Biden is the current president.' and the model response is 'The President of the United States is the head of state...', \"\n",
    "        f\"that is **Selective or Incomplete Retrieval**, as the response given by the model is correct, but does not exactly answer the question, and thus doesn't match the needle.\\n\"\n",
    "        f\"- If the model response is 'Elon Musk is the President of the United States.', that is a **Contradictory Hallucination**, where the response given by the model is completely incorrect.\\n\\n\"\n",
    "        f\"Return your output in this exact JSON format:\\n\"\n",
    "        f'{{\"model_response\": \"<text>\", \"needle\": \"<real_needle>\", \"hallucination_type\": \"<Selective or Incomplete Retrieval | Contradictory Hallucination>\"}}\\n\\n'\n",
    "        f\"---\\n\"\n",
    "        f\"Real Needle (correct answer): {real_needle}\\n\"\n",
    "        f\"Model Response: {model_response}\\n\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load .env and API key\n",
    "load_dotenv()\n",
    "groq_api_key = os.getenv(\"snlp_api_key\")\n",
    "\n",
    "# Initialize Groq client\n",
    "client = Groq(api_key=groq_api_key)\n",
    "\n",
    "def rewrite_question_and_needle(question, needle):\n",
    "    while True:\n",
    "        try:\n",
    "            prompt = build_rewrite_prompt(question, needle)\n",
    "            completion = client.chat.completions.create(\n",
    "                model=\"llama-3.3-70b-versatile\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=0.7,\n",
    "                max_tokens=512,\n",
    "                top_p=1.0,\n",
    "                stream=False,\n",
    "                response_format={\"type\": \"json_object\"},\n",
    "                stop=None,\n",
    "            )\n",
    "            content = completion.choices[0].message.content\n",
    "            data = json.loads(content)\n",
    "            return data[\"hallucination_type\"]\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            print(\"Retrying now...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       model_response      score  \\\n",
      "0   No, the FIFA World Cup will not be hosted this...  68.161678   \n",
      "1   No, the FIFA World Cup will not be hosted this...  64.018059   \n",
      "2   No, the FIFA World Cup will not be hosted this...  63.443971   \n",
      "3   No, the FIFA World Cup will not be hosted this...  61.143923   \n",
      "4   No. The FIFA World Cup is a quadrennial event....  52.484453   \n",
      "5   No. The 1942 and 1946 competitions, which Germ...  42.264229   \n",
      "6                                                 No.  11.441657   \n",
      "7      No, there won't be a FIFA World Cup this year.  84.071064   \n",
      "8   No, the FIFA World Cup will not be hosted this...  68.161678   \n",
      "9   No, the FIFA World Cup will not be hosted this...  64.754856   \n",
      "10  No, the FIFA World Cup will not be hosted this...  60.974330   \n",
      "11                                                No.  11.441657   \n",
      "12                                      No, it won't.  10.532115   \n",
      "13  No, the FIFA World Cup will not be held this y...  71.780235   \n",
      "14  No, the FIFA World Cup will not be hosted this...  68.161678   \n",
      "15  No, the FIFA World Cup will not be hosted this...  64.018059   \n",
      "16                                                No.  11.441657   \n",
      "17  No, the FIFA World Cup will not be held this y...  71.780235   \n",
      "18  No, there won't be a FIFA World Cup this year....  68.461436   \n",
      "19  No, the FIFA World Cup will not be hosted this...  68.161678   \n",
      "20                     No, it will be held next year.  42.366514   \n",
      "21                                                No.  11.441657   \n",
      "22                                      No, it won't.  10.532115   \n",
      "\n",
      "                                        needle  frequency  \\\n",
      "0   There won't be a FIFA World Cup this year.         10   \n",
      "1   There won't be a FIFA World Cup this year.          1   \n",
      "2   There won't be a FIFA World Cup this year.          1   \n",
      "3   There won't be a FIFA World Cup this year.          1   \n",
      "4   There won't be a FIFA World Cup this year.          1   \n",
      "5   There won't be a FIFA World Cup this year.          1   \n",
      "6   There won't be a FIFA World Cup this year.         10   \n",
      "7   There won't be a FIFA World Cup this year.          1   \n",
      "8   There won't be a FIFA World Cup this year.         18   \n",
      "9   There won't be a FIFA World Cup this year.          1   \n",
      "10  There won't be a FIFA World Cup this year.          1   \n",
      "11  There won't be a FIFA World Cup this year.          1   \n",
      "12  There won't be a FIFA World Cup this year.          3   \n",
      "13  There won't be a FIFA World Cup this year.          1   \n",
      "14  There won't be a FIFA World Cup this year.         14   \n",
      "15  There won't be a FIFA World Cup this year.          1   \n",
      "16  There won't be a FIFA World Cup this year.          9   \n",
      "17  There won't be a FIFA World Cup this year.          1   \n",
      "18  There won't be a FIFA World Cup this year.          1   \n",
      "19  There won't be a FIFA World Cup this year.         18   \n",
      "20  There won't be a FIFA World Cup this year.          1   \n",
      "21  There won't be a FIFA World Cup this year.          3   \n",
      "22  There won't be a FIFA World Cup this year.          1   \n",
      "\n",
      "                 category  \n",
      "0                relevant  \n",
      "1                relevant  \n",
      "2                relevant  \n",
      "3                relevant  \n",
      "4                relevant  \n",
      "5                relevant  \n",
      "6                relevant  \n",
      "7     relevant_misleading  \n",
      "8     relevant_misleading  \n",
      "9     relevant_misleading  \n",
      "10    relevant_misleading  \n",
      "11    relevant_misleading  \n",
      "12    relevant_misleading  \n",
      "13             irrelevant  \n",
      "14             irrelevant  \n",
      "15             irrelevant  \n",
      "16             irrelevant  \n",
      "17  irrelevant_misleading  \n",
      "18  irrelevant_misleading  \n",
      "19  irrelevant_misleading  \n",
      "20  irrelevant_misleading  \n",
      "21  irrelevant_misleading  \n",
      "22  irrelevant_misleading  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open(f\"unique_responses/{id_val}.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "rows = []\n",
    "for category, entries in data.items():\n",
    "    for entry in entries:\n",
    "        entry[\"category\"] = category\n",
    "        rows.append(entry)\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Refining Q&A pairs:   0%|          | 0/23 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Rate limit pause: sleeping for 15 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Refining Q&A pairs:  74%|███████▍  | 17/23 [00:17<00:03,  1.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Rate limit pause: sleeping for 15 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Refining Q&A pairs: 100%|██████████| 23/23 [00:33<00:00,  1.45s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "hallucination_types = []\n",
    "\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Refining Q&A pairs\"):\n",
    "    mr, n, s = row.get(\"model_response\", \"\"), row.get(\"needle\", \"\"), row.get(\"score\", -1)\n",
    "    if s > 50:\n",
    "        current_hallucination_type = \"No Hallucination\"\n",
    "    \n",
    "    else:\n",
    "        if idx > 0 and idx % 5 == 0:\n",
    "            print(\"⏳ Rate limit pause: sleeping for 15 seconds...\")\n",
    "            time.sleep(15)\n",
    "        \n",
    "        current_hallucination_type = rewrite_question_and_needle(mr, n)\n",
    "    hallucination_types.append(current_hallucination_type)\n",
    "\n",
    "# Update the DataFrame\n",
    "df['hallucination_type'] = hallucination_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by category and convert each group to a list of dicts\n",
    "json_output = {\n",
    "    category: group.drop(columns=\"category\").to_dict(orient=\"records\")\n",
    "    for category, group in df.groupby(\"category\")\n",
    "}\n",
    "\n",
    "output_dir = \"updated_unique_responses\"\n",
    "file_id = str(id_val)  # or whatever variable you're using\n",
    "output_path = os.path.join(output_dir, f\"{file_id}.json\")\n",
    "\n",
    "# Create the folder if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Write the JSON file into the folder\n",
    "with open(output_path, \"w\") as f:\n",
    "    json.dump(json_output, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
