{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview:\n",
    "\n",
    "This notebook processes and analyzes model test result JSON files to extract unique model responses, their scores and frequencies, and save the results for different context types. It includes functions to:\n",
    "\n",
    "- Load and process JSON files for a given context and ID.\n",
    "- Extract unique responses, their scores, and frequencies.\n",
    "- Save the processed data for all context types into a single JSON file for a specified ID.\n",
    "\n",
    "Additionally, it demonstrates saving the results for a specific ID as an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and Process JSON Files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from groq import Groq\n",
    "\n",
    "\n",
    "\n",
    "def __get_unique_model_responses(id: int, context_type: str, with_misleading: bool = False) -> list:\n",
    "    \"\"\"\n",
    "    Load and process model test result JSON files for a given context and ID.\n",
    "\n",
    "    Returns a list of unique model responses with their:\n",
    "    - score (from first occurrence)\n",
    "    - frequency (how many times that response appeared)\n",
    "    \n",
    "    Sorted by descending score.\n",
    "    \"\"\"\n",
    "    model_name = 'llama-2-7b-80k'\n",
    "    base_dir = Path('../results/graph')\n",
    "    \n",
    "    suffix = '_misleading' if with_misleading else ''\n",
    "    directory = base_dir / f'{model_name}_id_{id}_{context_type}{suffix}'\n",
    "\n",
    "    # Collect all responses and scores\n",
    "    data = [\n",
    "        json.load(open(directory / file, encoding='utf-8'))\n",
    "        for file in os.listdir(directory) if file.endswith('.json')\n",
    "    ]\n",
    "    \n",
    "    df = pd.DataFrame(data)[['model_response', 'needle', 'context_length', 'depth_percent']]\n",
    "    #question_df = pd.read_excel(\"../data_generation/FreshQADataset_with_misleading_combined.xlsx\")\n",
    "    question_df = pd.read_json(\"../data_generation/context_cleaned.json\")\n",
    "    try:\n",
    "        question = question_df.set_index(\"id\").at[id, \"question\"]\n",
    "        df['question'] = question\n",
    "        \n",
    "    except KeyError:\n",
    "        raise ValueError(f\"No question found for id: {id}\")\n",
    "\n",
    "    # Compute frequency\n",
    "    freq = df['model_response'].value_counts().rename('frequency')\n",
    "\n",
    "    # Drop duplicates but keep first score\n",
    "    # df = df.drop_duplicates(subset='model_response')\n",
    "\n",
    "    # Merge with frequency\n",
    "    df = df.merge(freq, left_on='model_response', right_index=True)\n",
    "\n",
    "    # Sort by score descending\n",
    "    # df = df.sort_values(by='score', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    return df.to_dict(orient='records')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Processed Results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_all_contexts_for_id(id: int) -> None:\n",
    "    \"\"\"\n",
    "    Gather results from all 4 context types and save them in a single JSON file for the given id.\n",
    "\n",
    "    File will be saved as: unique_responses/{id}.json\n",
    "    \"\"\"\n",
    "    save_dir = Path('unique_responses')\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    all_data = {\n",
    "        'relevant': __get_unique_model_responses(id, 'relevant', with_misleading=False),\n",
    "        'relevant_misleading': __get_unique_model_responses(id, 'relevant', with_misleading=True),\n",
    "        'irrelevant': __get_unique_model_responses(id, 'irrelevant', with_misleading=False),\n",
    "        'irrelevant_misleading': __get_unique_model_responses(id, 'irrelevant', with_misleading=True),\n",
    "    }\n",
    "\n",
    "    save_path = save_dir / f'{id}.json'\n",
    "    with open(save_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(all_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"Saved all context responses for ID {id} to {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 11, 121, 122, 123, 124, 14, 15, 155, 16, 160, 163, 164, 167, 168, 17, 170, 172, 173, 175, 18, 180, 182, 183, 185, 189, 192, 193, 196, 198, 2, 29, 3, 380, 391, 393, 396, 4, 401, 403, 408, 409, 410, 411, 420, 421, 422, 423, 424, 427, 43, 430, 432, 433, 435, 436, 437, 438, 439, 44, 441, 442, 444, 448, 449, 453, 5, 532, 535, 539, 576, 577, 578, 579, 586, 587, 588, 589, 590, 92, 95, 96]\n"
     ]
    }
   ],
   "source": [
    "# id_val = 44\n",
    "ids = []\n",
    "for file_name in os.listdir('../haystack/irrelevant'):\n",
    "    if file_name.endswith('.txt'):\n",
    "        file_id = os.path.splitext(file_name)[0]  # Extract the file name without extension\n",
    "        ids.append(int(file_id))\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved all context responses for ID 1 to unique_responses\\1.json\n",
      "Saved all context responses for ID 11 to unique_responses\\11.json\n",
      "Saved all context responses for ID 121 to unique_responses\\121.json\n",
      "Saved all context responses for ID 122 to unique_responses\\122.json\n",
      "Saved all context responses for ID 123 to unique_responses\\123.json\n",
      "Saved all context responses for ID 124 to unique_responses\\124.json\n",
      "Saved all context responses for ID 14 to unique_responses\\14.json\n",
      "Saved all context responses for ID 15 to unique_responses\\15.json\n",
      "Saved all context responses for ID 155 to unique_responses\\155.json\n",
      "Saved all context responses for ID 16 to unique_responses\\16.json\n",
      "Saved all context responses for ID 160 to unique_responses\\160.json\n",
      "Saved all context responses for ID 163 to unique_responses\\163.json\n",
      "Saved all context responses for ID 164 to unique_responses\\164.json\n",
      "Saved all context responses for ID 167 to unique_responses\\167.json\n",
      "Saved all context responses for ID 168 to unique_responses\\168.json\n",
      "Saved all context responses for ID 17 to unique_responses\\17.json\n",
      "Saved all context responses for ID 170 to unique_responses\\170.json\n",
      "Saved all context responses for ID 172 to unique_responses\\172.json\n",
      "Saved all context responses for ID 173 to unique_responses\\173.json\n",
      "Saved all context responses for ID 175 to unique_responses\\175.json\n",
      "Saved all context responses for ID 18 to unique_responses\\18.json\n",
      "Saved all context responses for ID 180 to unique_responses\\180.json\n",
      "Saved all context responses for ID 182 to unique_responses\\182.json\n",
      "Saved all context responses for ID 183 to unique_responses\\183.json\n",
      "Saved all context responses for ID 185 to unique_responses\\185.json\n",
      "Saved all context responses for ID 189 to unique_responses\\189.json\n",
      "Saved all context responses for ID 192 to unique_responses\\192.json\n",
      "Saved all context responses for ID 193 to unique_responses\\193.json\n",
      "Saved all context responses for ID 196 to unique_responses\\196.json\n",
      "Saved all context responses for ID 198 to unique_responses\\198.json\n",
      "Saved all context responses for ID 2 to unique_responses\\2.json\n",
      "Saved all context responses for ID 29 to unique_responses\\29.json\n",
      "Saved all context responses for ID 3 to unique_responses\\3.json\n",
      "Saved all context responses for ID 380 to unique_responses\\380.json\n",
      "Saved all context responses for ID 391 to unique_responses\\391.json\n",
      "Saved all context responses for ID 393 to unique_responses\\393.json\n",
      "Saved all context responses for ID 396 to unique_responses\\396.json\n",
      "Saved all context responses for ID 4 to unique_responses\\4.json\n",
      "Saved all context responses for ID 401 to unique_responses\\401.json\n",
      "Saved all context responses for ID 403 to unique_responses\\403.json\n",
      "Saved all context responses for ID 408 to unique_responses\\408.json\n",
      "Saved all context responses for ID 409 to unique_responses\\409.json\n",
      "Saved all context responses for ID 410 to unique_responses\\410.json\n",
      "Saved all context responses for ID 411 to unique_responses\\411.json\n",
      "Saved all context responses for ID 420 to unique_responses\\420.json\n",
      "Saved all context responses for ID 421 to unique_responses\\421.json\n",
      "Saved all context responses for ID 422 to unique_responses\\422.json\n",
      "Saved all context responses for ID 423 to unique_responses\\423.json\n",
      "Saved all context responses for ID 424 to unique_responses\\424.json\n",
      "Saved all context responses for ID 427 to unique_responses\\427.json\n",
      "Saved all context responses for ID 43 to unique_responses\\43.json\n",
      "Saved all context responses for ID 430 to unique_responses\\430.json\n",
      "Saved all context responses for ID 432 to unique_responses\\432.json\n",
      "Saved all context responses for ID 433 to unique_responses\\433.json\n",
      "Saved all context responses for ID 435 to unique_responses\\435.json\n",
      "Saved all context responses for ID 436 to unique_responses\\436.json\n",
      "Saved all context responses for ID 437 to unique_responses\\437.json\n",
      "Saved all context responses for ID 438 to unique_responses\\438.json\n",
      "Saved all context responses for ID 439 to unique_responses\\439.json\n",
      "Saved all context responses for ID 44 to unique_responses\\44.json\n",
      "Saved all context responses for ID 441 to unique_responses\\441.json\n",
      "Saved all context responses for ID 442 to unique_responses\\442.json\n",
      "Saved all context responses for ID 444 to unique_responses\\444.json\n",
      "Saved all context responses for ID 448 to unique_responses\\448.json\n",
      "Saved all context responses for ID 449 to unique_responses\\449.json\n",
      "Saved all context responses for ID 453 to unique_responses\\453.json\n",
      "Saved all context responses for ID 5 to unique_responses\\5.json\n",
      "Saved all context responses for ID 532 to unique_responses\\532.json\n",
      "Saved all context responses for ID 535 to unique_responses\\535.json\n",
      "Saved all context responses for ID 539 to unique_responses\\539.json\n",
      "Saved all context responses for ID 576 to unique_responses\\576.json\n",
      "Saved all context responses for ID 577 to unique_responses\\577.json\n",
      "Saved all context responses for ID 578 to unique_responses\\578.json\n",
      "Saved all context responses for ID 579 to unique_responses\\579.json\n",
      "Saved all context responses for ID 586 to unique_responses\\586.json\n",
      "Saved all context responses for ID 587 to unique_responses\\587.json\n",
      "Saved all context responses for ID 588 to unique_responses\\588.json\n",
      "Saved all context responses for ID 589 to unique_responses\\589.json\n",
      "Saved all context responses for ID 590 to unique_responses\\590.json\n",
      "Saved all context responses for ID 92 to unique_responses\\92.json\n",
      "Saved all context responses for ID 95 to unique_responses\\95.json\n",
      "Saved all context responses for ID 96 to unique_responses\\96.json\n"
     ]
    }
   ],
   "source": [
    "for i in ids:\n",
    "    save_all_contexts_for_id(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# __get_unique_model_responses(ids, 'relevant', with_misleading=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rewrite_prompt(model_response, real_needle, question):\n",
    "    return (\n",
    "        f\"\"\"You are evaluating a model-generated response for hallucination analysis in a long-context retrieval setting.\n",
    "\n",
    "            You are given the following:\n",
    "\n",
    "            Question: {question}\n",
    "\n",
    "            The correct answer (real needle): {real_needle}\n",
    "\n",
    "            A model-generated response: {model_response}\n",
    "\n",
    "            Your task is to classify the model's response into one of the following categories:\n",
    "\n",
    "            1. Correct Retrieval: the model answered the question correctly\n",
    "\n",
    "            2. Excess Incorrect Retrieval: the model answered the question correctly but it also went on to output other text that was either completely irrelevant or invalid with regards to the question.\n",
    "\n",
    "            3. Irrelevant Retrieval: the model did not answer the question, it went on to output other text\n",
    "\n",
    "            4. Contradictory Retrieval: the model answered the question incorrectly\n",
    "\n",
    "            Examples:\n",
    "\n",
    "            Question: Who is the President of the United States? Needle: The President of the United States is Donald Trump.\n",
    "\n",
    "            1: The President of the United States is Donald Trump. (Correct Retrieval)\n",
    "\n",
    "            2: The President of the United States is Donald Trump. The affirmation was made in the Declaration of Independence, which was written predominantly by Thomas Jefferson in 1776. (Excess Incorrect Retrieval)\n",
    "\n",
    "            3: The President of the United States is the head of state and head of government of the United States. The president directs the executive branch of the federal government and is the commander-in-chief of the United States Armed Forces. (Irrelevant Retrieval)\n",
    "\n",
    "            4: Joe Biden is the President of the United States. (Contradictory Retrieval)\n",
    "\n",
    "            Return your output in this exact JSON format: {{\"model_response\": \"<text>\", \"needle\": \"<real_needle>\", \"hallucination_type\": \"<Correct Retrieval | Excess Incorrect Retrieval | Irrelevant Retrieval | Contradictory Retrieval>\"}}\"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "# Load .env and API key\n",
    "load_dotenv()\n",
    "groq_api_key = os.getenv(\"snlp_api_key\")\n",
    "print(groq_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize Groq client\n",
    "# client = Groq(api_key=groq_api_key)\n",
    "\n",
    "# def hallucination_categorising(response, needle, question):\n",
    "#     while True:\n",
    "#         try:\n",
    "#             prompt = build_rewrite_prompt(response, needle, question)\n",
    "#             completion = client.chat.completions.create(\n",
    "#                 model=\"llama-3.3-70b-versatile\",\n",
    "#                 messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "#                 temperature=0.7,\n",
    "#                 max_tokens=512,\n",
    "#                 top_p=1.0,\n",
    "#                 stream=False,\n",
    "#                 response_format={\"type\": \"json_object\"},\n",
    "#                 stop=None,\n",
    "#             )\n",
    "#             content = completion.choices[0].message.content\n",
    "#             data = json.loads(content)\n",
    "#             return data[\"hallucination_type\"]\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error: {e}\")\n",
    "#             print(\"Retrying now...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from together import Together\n",
    "\n",
    "client = Together(api_key=\"1fedf53fbab9628b7eae47f56d2bea3773754ece7206aa81a6faf83e73e11eb6\")\n",
    "\n",
    "def hallucination_categorising(response, needle, question):\n",
    "    while True:\n",
    "        try:\n",
    "            prompt = build_rewrite_prompt(response, needle, question)\n",
    "            completion = client.chat.completions.create(\n",
    "                model=\"meta-llama/Llama-3.3-70B-Instruct-Turbo\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=0.7,\n",
    "                max_tokens=512,\n",
    "                top_p=1.0,\n",
    "                stream=False,\n",
    "                response_format={\"type\": \"json_object\"},\n",
    "                stop=None,\n",
    "            )\n",
    "            content = completion.choices[0].message.content\n",
    "            data = json.loads(content)\n",
    "            return data[\"hallucination_type\"]\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            print(\"Retrying now...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_dfs = []\n",
    "for i in ids:\n",
    "    with open(f\"unique_responses/{i}.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "        \n",
    "    rows = []\n",
    "    for category, entries in data.items():\n",
    "        for entry in entries:\n",
    "            entry[\"category\"] = category\n",
    "            rows.append(entry)\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    # print(df)\n",
    "    lst_dfs.append(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82\n"
     ]
    }
   ],
   "source": [
    "print(len(lst_dfs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Categorising hallucinations: 100%|██████████| 100/100 [01:52<00:00,  1.12s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [01:55<00:00,  1.16s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [02:04<00:00,  1.24s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [08:48<00:00,  5.29s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [05:24<00:00,  3.24s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [02:11<00:00,  1.31s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [02:13<00:00,  1.33s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [02:49<00:00,  1.70s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [04:07<00:00,  2.47s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [02:54<00:00,  1.75s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [01:48<00:00,  1.08s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [01:54<00:00,  1.14s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [02:00<00:00,  1.21s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [01:53<00:00,  1.13s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [02:32<00:00,  1.52s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [02:51<00:00,  1.71s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [02:34<00:00,  1.54s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [02:17<00:00,  1.38s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [02:32<00:00,  1.52s/it]\n",
      "Categorising hallucinations:  90%|█████████ | 90/100 [02:08<00:15,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Expecting ':' delimiter: line 2437 column 1 (char 3002)\n",
      "Retrying now...\n",
      "Error: Expecting ',' delimiter: line 1 column 1161 (char 1160)\n",
      "Retrying now...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Categorising hallucinations: 100%|██████████| 100/100 [03:07<00:00,  1.88s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [04:05<00:00,  2.45s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [05:04<00:00,  3.04s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [02:04<00:00,  1.25s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [01:52<00:00,  1.13s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [02:47<00:00,  1.67s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [01:59<00:00,  1.20s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [01:55<00:00,  1.16s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [01:52<00:00,  1.13s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [01:53<00:00,  1.13s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [02:22<00:00,  1.42s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [03:09<00:00,  1.90s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [03:40<00:00,  2.21s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [03:53<00:00,  2.33s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [03:53<00:00,  2.34s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [03:02<00:00,  1.83s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [02:35<00:00,  1.55s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [03:35<00:00,  2.15s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [03:14<00:00,  1.95s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [03:56<00:00,  2.37s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [05:28<00:00,  3.28s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [02:43<00:00,  1.64s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [02:28<00:00,  1.48s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [08:57<00:00,  5.38s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [02:59<00:00,  1.80s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [03:06<00:00,  1.87s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [03:50<00:00,  2.31s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [04:33<00:00,  2.74s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [05:38<00:00,  3.39s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [04:34<00:00,  2.74s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [03:12<00:00,  1.93s/it]\n",
      "Categorising hallucinations:  17%|█▋        | 17/100 [00:51<05:14,  3.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 'hallucination_type'\n",
      "Retrying now...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Categorising hallucinations: 100%|██████████| 100/100 [04:30<00:00,  2.70s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [03:04<00:00,  1.85s/it]\n",
      "Categorising hallucinations:  34%|███▍      | 34/100 [00:50<01:35,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Expecting ',' delimiter: line 921 column 1 (char 1603)\n",
      "Retrying now...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Categorising hallucinations: 100%|██████████| 100/100 [02:40<00:00,  1.61s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [04:00<00:00,  2.41s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [04:03<00:00,  2.44s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [02:37<00:00,  1.58s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [02:43<00:00,  1.63s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [02:48<00:00,  1.69s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [03:18<00:00,  1.98s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [04:03<00:00,  2.43s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [03:25<00:00,  2.05s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [01:50<00:00,  1.10s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [03:06<00:00,  1.87s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [02:59<00:00,  1.80s/it]\n",
      "Categorising hallucinations:  30%|███       | 30/100 [01:11<02:39,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 'hallucination_type'\n",
      "Retrying now...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Categorising hallucinations: 100%|██████████| 100/100 [03:31<00:00,  2.11s/it]\n",
      "Categorising hallucinations:  40%|████      | 40/100 [01:26<02:22,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Expecting value: line 1 column 19 (char 18)\n",
      "Retrying now...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Categorising hallucinations: 100%|██████████| 100/100 [03:32<00:00,  2.12s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [03:49<00:00,  2.30s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [02:48<00:00,  1.69s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [02:40<00:00,  1.60s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [03:51<00:00,  2.31s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [03:20<00:00,  2.00s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [03:36<00:00,  2.17s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [03:29<00:00,  2.10s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [03:40<00:00,  2.21s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [02:20<00:00,  1.41s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [02:37<00:00,  1.58s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [02:48<00:00,  1.69s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [02:34<00:00,  1.55s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [03:01<00:00,  1.82s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [06:16<00:00,  3.77s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [04:56<00:00,  2.96s/it]\n",
      "Categorising hallucinations: 100%|██████████| 100/100 [02:29<00:00,  1.49s/it]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "for single_df in lst_dfs:\n",
    "    hallucination_types = []\n",
    "\n",
    "    for idx, row in tqdm(single_df.iterrows(), total=len(single_df), desc=\"Categorising hallucinations\"):\n",
    "        start_time = time.time()  # ⏱ Track time at the start of each request\n",
    "\n",
    "        mr = row.get(\"model_response\", \"\")\n",
    "        n = row.get(\"needle\", \"\")\n",
    "        q = row.get(\"question\", \"\")\n",
    "\n",
    "        current_hallucination_type = hallucination_categorising(mr, n, q)\n",
    "        hallucination_types.append(current_hallucination_type)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "        sleep_time = max(0, 1.0 - elapsed)  # ⏸ Sleep the remaining time to enforce 1 req/sec\n",
    "        time.sleep(sleep_time)\n",
    "\n",
    "    single_df['hallucination_type'] = hallucination_types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df_single, id_single in zip(lst_dfs, ids):\n",
    "    # Group by category and convert each group to a list of dicts\n",
    "    json_output = {\n",
    "        category: group.drop(columns=\"category\").to_dict(orient=\"records\")\n",
    "        for category, group in df_single.groupby(\"category\")\n",
    "    }\n",
    "\n",
    "    output_dir = \"updated_unique_responses\"\n",
    "    file_id = str(id_single)  # or whatever variable you're using\n",
    "    output_path = os.path.join(output_dir, f\"{file_id}.json\")\n",
    "\n",
    "    # Create the folder if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Write the JSON file into the folder\n",
    "    with open(output_path, \"w\") as f:\n",
    "        json.dump(json_output, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
