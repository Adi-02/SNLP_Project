{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "851582af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "74f7126a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"yaofu/Llama-2-7b-80k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8819413b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['410.txt', '96.txt', '95.txt', '92.txt', '590.txt', '409.txt', '588.txt', '170.txt', '586.txt', '579.txt', '436.txt', '577.txt', '576.txt', '539.txt', '535.txt', '1.txt', '532.txt', '453.txt', '449.txt', '448.txt', '11.txt', '444.txt', '442.txt', '441.txt', '14.txt', '439.txt', '438.txt', '437.txt', '403.txt', '435.txt', '433.txt', '432.txt', '430.txt', '183.txt', '15.txt', '427.txt', '16.txt', '423.txt', '422.txt', '421.txt', '420.txt', '17.txt', '18.txt', '2.txt', '411.txt', '408.txt', '124.txt', '396.txt', '29.txt', '578.txt', '401.txt', '3.txt', '180.txt', '4.txt', '5.txt', '393.txt', '391.txt', '380.txt', '198.txt', '196.txt', '193.txt', '192.txt', '189.txt', '44.txt', '185.txt', '587.txt', '182.txt', '43.txt', '175.txt', '173.txt', '172.txt', '424.txt', '168.txt', '167.txt', '164.txt', '163.txt', '160.txt', '155.txt', '589.txt', '123.txt', '122.txt', '121.txt']\n",
      "../haystack/irrelevant/410.txt\n",
      "../haystack/irrelevant/96.txt\n",
      "../haystack/irrelevant/95.txt\n",
      "../haystack/irrelevant/92.txt\n",
      "../haystack/irrelevant/590.txt\n",
      "../haystack/irrelevant/409.txt\n",
      "../haystack/irrelevant/588.txt\n",
      "../haystack/irrelevant/170.txt\n",
      "../haystack/irrelevant/586.txt\n",
      "../haystack/irrelevant/579.txt\n",
      "../haystack/irrelevant/436.txt\n",
      "../haystack/irrelevant/577.txt\n",
      "../haystack/irrelevant/576.txt\n",
      "../haystack/irrelevant/539.txt\n",
      "../haystack/irrelevant/535.txt\n",
      "../haystack/irrelevant/1.txt\n",
      "../haystack/irrelevant/532.txt\n",
      "../haystack/irrelevant/453.txt\n",
      "../haystack/irrelevant/449.txt\n",
      "../haystack/irrelevant/448.txt\n",
      "../haystack/irrelevant/11.txt\n",
      "../haystack/irrelevant/444.txt\n",
      "../haystack/irrelevant/442.txt\n",
      "../haystack/irrelevant/441.txt\n",
      "../haystack/irrelevant/14.txt\n",
      "../haystack/irrelevant/439.txt\n",
      "../haystack/irrelevant/438.txt\n",
      "../haystack/irrelevant/437.txt\n",
      "../haystack/irrelevant/403.txt\n",
      "../haystack/irrelevant/435.txt\n",
      "../haystack/irrelevant/433.txt\n",
      "../haystack/irrelevant/432.txt\n",
      "../haystack/irrelevant/430.txt\n",
      "../haystack/irrelevant/183.txt\n",
      "../haystack/irrelevant/15.txt\n",
      "../haystack/irrelevant/427.txt\n",
      "../haystack/irrelevant/16.txt\n",
      "../haystack/irrelevant/423.txt\n",
      "../haystack/irrelevant/422.txt\n",
      "../haystack/irrelevant/421.txt\n",
      "../haystack/irrelevant/420.txt\n",
      "../haystack/irrelevant/17.txt\n",
      "../haystack/irrelevant/18.txt\n",
      "../haystack/irrelevant/2.txt\n",
      "../haystack/irrelevant/411.txt\n",
      "../haystack/irrelevant/408.txt\n",
      "../haystack/irrelevant/124.txt\n",
      "../haystack/irrelevant/396.txt\n",
      "../haystack/irrelevant/29.txt\n",
      "../haystack/irrelevant/578.txt\n",
      "../haystack/irrelevant/401.txt\n",
      "../haystack/irrelevant/3.txt\n",
      "../haystack/irrelevant/180.txt\n",
      "../haystack/irrelevant/4.txt\n",
      "../haystack/irrelevant/5.txt\n",
      "../haystack/irrelevant/393.txt\n",
      "../haystack/irrelevant/391.txt\n",
      "../haystack/irrelevant/380.txt\n",
      "../haystack/irrelevant/198.txt\n",
      "../haystack/irrelevant/196.txt\n",
      "../haystack/irrelevant/193.txt\n",
      "../haystack/irrelevant/192.txt\n",
      "../haystack/irrelevant/189.txt\n",
      "../haystack/irrelevant/44.txt\n",
      "../haystack/irrelevant/185.txt\n",
      "../haystack/irrelevant/587.txt\n",
      "../haystack/irrelevant/182.txt\n",
      "../haystack/irrelevant/43.txt\n",
      "../haystack/irrelevant/175.txt\n",
      "../haystack/irrelevant/173.txt\n",
      "../haystack/irrelevant/172.txt\n",
      "../haystack/irrelevant/424.txt\n",
      "../haystack/irrelevant/168.txt\n",
      "../haystack/irrelevant/167.txt\n",
      "../haystack/irrelevant/164.txt\n",
      "../haystack/irrelevant/163.txt\n",
      "../haystack/irrelevant/160.txt\n",
      "../haystack/irrelevant/155.txt\n",
      "../haystack/irrelevant/589.txt\n",
      "../haystack/irrelevant/123.txt\n",
      "../haystack/irrelevant/122.txt\n",
      "../haystack/irrelevant/121.txt\n",
      "['410.txt', '96.txt', '95.txt', '92.txt', '590.txt', '409.txt', '588.txt', '170.txt', '586.txt', '579.txt', '436.txt', '577.txt', '576.txt', '539.txt', '535.txt', '1.txt', '532.txt', '453.txt', '449.txt', '448.txt', '11.txt', '444.txt', '442.txt', '441.txt', '14.txt', '439.txt', '438.txt', '437.txt', '403.txt', '435.txt', '433.txt', '432.txt', '430.txt', '183.txt', '15.txt', '427.txt', '16.txt', '423.txt', '422.txt', '421.txt', '420.txt', '17.txt', '18.txt', '2.txt', '411.txt', '408.txt', '124.txt', '396.txt', '29.txt', '578.txt', '401.txt', '3.txt', '180.txt', '4.txt', '5.txt', '393.txt', '391.txt', '380.txt', '198.txt', '196.txt', '193.txt', '192.txt', '189.txt', '44.txt', '185.txt', '587.txt', '182.txt', '43.txt', '175.txt', '173.txt', '172.txt', '424.txt', '168.txt', '167.txt', '164.txt', '163.txt', '160.txt', '155.txt', '589.txt', '123.txt', '122.txt', '121.txt']\n",
      "../haystack/relevant/410.txt\n",
      "../haystack/relevant/96.txt\n",
      "../haystack/relevant/95.txt\n",
      "../haystack/relevant/92.txt\n",
      "../haystack/relevant/590.txt\n",
      "../haystack/relevant/409.txt\n",
      "../haystack/relevant/588.txt\n",
      "../haystack/relevant/170.txt\n",
      "../haystack/relevant/586.txt\n",
      "../haystack/relevant/579.txt\n",
      "../haystack/relevant/436.txt\n",
      "../haystack/relevant/577.txt\n",
      "../haystack/relevant/576.txt\n",
      "../haystack/relevant/539.txt\n",
      "../haystack/relevant/535.txt\n",
      "../haystack/relevant/1.txt\n",
      "../haystack/relevant/532.txt\n",
      "../haystack/relevant/453.txt\n",
      "../haystack/relevant/449.txt\n",
      "../haystack/relevant/448.txt\n",
      "../haystack/relevant/11.txt\n",
      "../haystack/relevant/444.txt\n",
      "../haystack/relevant/442.txt\n",
      "../haystack/relevant/441.txt\n",
      "../haystack/relevant/14.txt\n",
      "../haystack/relevant/439.txt\n",
      "../haystack/relevant/438.txt\n",
      "../haystack/relevant/437.txt\n",
      "../haystack/relevant/403.txt\n",
      "../haystack/relevant/435.txt\n",
      "../haystack/relevant/433.txt\n",
      "../haystack/relevant/432.txt\n",
      "../haystack/relevant/430.txt\n",
      "../haystack/relevant/183.txt\n",
      "../haystack/relevant/15.txt\n",
      "../haystack/relevant/427.txt\n",
      "../haystack/relevant/16.txt\n",
      "../haystack/relevant/423.txt\n",
      "../haystack/relevant/422.txt\n",
      "../haystack/relevant/421.txt\n",
      "../haystack/relevant/420.txt\n",
      "../haystack/relevant/17.txt\n",
      "../haystack/relevant/18.txt\n",
      "../haystack/relevant/2.txt\n",
      "../haystack/relevant/411.txt\n",
      "../haystack/relevant/408.txt\n",
      "../haystack/relevant/124.txt\n",
      "../haystack/relevant/396.txt\n",
      "../haystack/relevant/29.txt\n",
      "../haystack/relevant/578.txt\n",
      "../haystack/relevant/401.txt\n",
      "../haystack/relevant/3.txt\n",
      "../haystack/relevant/180.txt\n",
      "../haystack/relevant/4.txt\n",
      "../haystack/relevant/5.txt\n",
      "../haystack/relevant/393.txt\n",
      "../haystack/relevant/391.txt\n",
      "../haystack/relevant/380.txt\n",
      "../haystack/relevant/198.txt\n",
      "../haystack/relevant/196.txt\n",
      "../haystack/relevant/193.txt\n",
      "../haystack/relevant/192.txt\n",
      "../haystack/relevant/189.txt\n",
      "../haystack/relevant/44.txt\n",
      "../haystack/relevant/185.txt\n",
      "../haystack/relevant/587.txt\n",
      "../haystack/relevant/182.txt\n",
      "../haystack/relevant/43.txt\n",
      "../haystack/relevant/175.txt\n",
      "../haystack/relevant/173.txt\n",
      "../haystack/relevant/172.txt\n",
      "../haystack/relevant/424.txt\n",
      "../haystack/relevant/168.txt\n",
      "../haystack/relevant/167.txt\n",
      "../haystack/relevant/164.txt\n",
      "../haystack/relevant/163.txt\n",
      "../haystack/relevant/160.txt\n",
      "../haystack/relevant/155.txt\n",
      "../haystack/relevant/589.txt\n",
      "../haystack/relevant/123.txt\n",
      "../haystack/relevant/122.txt\n",
      "../haystack/relevant/121.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to tokenize and count tokens in files within a directory\n",
    "def tokenize_and_count(directory):\n",
    "    token_counts = []\n",
    "    print(os.listdir(directory))\n",
    "    for filename in os.listdir(directory):\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        print(filepath)\n",
    "        if os.path.isfile(filepath):\n",
    "            with open(filepath, 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "                tokens = tokenizer.tokenize(text)\n",
    "                token_counts.append(len(tokens))\n",
    "    return token_counts\n",
    "\n",
    "# Tokenize and count tokens for irrelevant and relevant directories\n",
    "irrelevant_dir = '../haystack/irrelevant'\n",
    "relevant_dir = '../haystack/relevant'\n",
    "\n",
    "irrelevant_token_counts = tokenize_and_count(irrelevant_dir)\n",
    "relevant_token_counts = tokenize_and_count(relevant_dir)\n",
    "\n",
    "# # Plot the distribution of tokens\n",
    "# plt.figure(figsize=(12, 6))\n",
    "\n",
    "# plt.hist(irrelevant_token_counts, bins=20, alpha=0.7, label='Irrelevant', color='red')\n",
    "\n",
    "# plt.title('Token Distribution')\n",
    "# plt.xlabel('Number of Tokens')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bb8571fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82\n",
      "82\n"
     ]
    }
   ],
   "source": [
    "# Function to tokenize and count tokens in files within a directory, returning file names as well\n",
    "def tokenize_and_count_with_filenames(directory):\n",
    "    token_counts = []\n",
    "    file_names = []\n",
    "    for filename in os.listdir(directory):\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        if os.path.isfile(filepath):\n",
    "            with open(filepath, 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "                tokens = tokenizer.tokenize(text)\n",
    "                token_counts.append(len(tokens))\n",
    "                file_names.append(filename)\n",
    "    return token_counts, file_names\n",
    "\n",
    "# Get token counts and file names for irrelevant and relevant directories\n",
    "irrelevant_token_counts, irrelevant_file_names = tokenize_and_count_with_filenames(irrelevant_dir)\n",
    "print(len(irrelevant_token_counts))\n",
    "relevant_token_counts, relevant_file_names = tokenize_and_count_with_filenames(relevant_dir)\n",
    "print(len(relevant_token_counts))\n",
    "\n",
    "# Get file names with less than 5000 tokens\n",
    "irrelevant_files_under_5000 = [irrelevant_file_names[i] for i, count in enumerate(irrelevant_token_counts) if count < 5000]\n",
    "relevant_files_under_5000 = [relevant_file_names[i] for i, count in enumerate(relevant_token_counts) if count < 5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dd95c4ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Irrelevant files with less than 5000 tokens: []\n",
      "Relevant files with less than 5000 tokens: []\n"
     ]
    }
   ],
   "source": [
    "print(\"Irrelevant files with less than 5000 tokens:\", irrelevant_files_under_5000)\n",
    "print(\"Relevant files with less than 5000 tokens:\", relevant_files_under_5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856ee399",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_to_dup = set(irrelevant_files_under_5000).union(set(relevant_files_under_5000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0bb0aa6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for id in relevant_files_under_5000:\n",
    "    r_file_path = relevant_dir + f\"/{id}\"  # Change this to your file name\n",
    "\n",
    "    # Step 1: Read the original content\n",
    "    with open(r_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        content = f.read()\n",
    "\n",
    "    # Step 2: Duplicate the content\n",
    "    duplicated_content = content + content  # or content * 2\n",
    "\n",
    "    # Step 3: Write it back into the same file\n",
    "    with open(r_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(duplicated_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "544b65ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "for id in irrelevant_files_under_5000:\n",
    "    irr_file_path = irrelevant_dir + f\"/{id}\"  # Change this to your file name\n",
    "\n",
    "    # Step 1: Read the original content\n",
    "    with open(irr_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        content = f.read()\n",
    "\n",
    "    # Step 2: Duplicate the content\n",
    "    duplicated_content = content + content  # or content * 2\n",
    "\n",
    "    # Step 3: Write it back into the same file\n",
    "    with open(irr_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(duplicated_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0fadd14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in files_to_remove:\n",
    "#     os.remove(os.path.join(irrelevant_dir, i))\n",
    "#     os.remove(os.path.join(relevant_dir, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d52c4ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# j = 0\n",
    "# for i in files_to_remove:\n",
    "#     if not os.path.exists(os.path.join(irrelevant_dir, i)):\n",
    "#         print(f\"Removed {i} from irrelevant directory\")\n",
    "#         j += 1\n",
    "# print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "43cc69a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(files_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "affa1337",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      2\u001b[0m json_file \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_json(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../haystack/needles.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m json_file\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "json_file = pd.read_json(\"../haystack/needles.json\")\n",
    "json_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0481950",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = json_file[~json_file['context_irrelevant'].isin(files_to_remove)]\n",
    "json_file = json_file[~json_file['context_relevant'].isin(files_to_remove)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1706a58e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(82, 12)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_file.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21766cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_output = json_file.to_json(orient=\"records\", indent=4, force_ascii=False)\n",
    "with open(\"../haystack/needles.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(json_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fcf05f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
