{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embedding model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def remove_similar_sentences(context, needle, threshold=0.8):\n",
    "    sentences = sent_tokenize(context)\n",
    "    needle_embedding = model.encode(needle, convert_to_tensor=True)\n",
    "    sentence_embeddings = model.encode(sentences, convert_to_tensor=True)\n",
    "    cosine_scores = util.cos_sim(needle_embedding, sentence_embeddings)[0]\n",
    "\n",
    "    removed = []\n",
    "    kept = []\n",
    "\n",
    "    for sent, score in zip(sentences, cosine_scores):\n",
    "        if score >= threshold:\n",
    "            removed.append(sent)\n",
    "        else:\n",
    "            kept.append(sent)\n",
    "\n",
    "    return ' '.join(kept), removed\n",
    "\n",
    "def clean_files(df, column_name):\n",
    "    \"\"\" This uses above function to actually remove all the sentences from the context files \"\"\"\n",
    "    # Loop through your DataFrame rows\n",
    "    for idx, row in df.iterrows():\n",
    "        \n",
    "        file_name = row[column_name]  # assumes you have full or relative path here\n",
    "        if column_name == 'context_relevant':\n",
    "            context_dir = os.path.join(\"..\", \"haystack\", \"relevant\")\n",
    "        else:\n",
    "            context_dir = os.path.join(\"..\", \"haystack\", \"irrelevant\")\n",
    "            \n",
    "        file_path = os.path.join(context_dir, file_name)\n",
    "        needle = row['needle']\n",
    "\n",
    "        # Check file exists\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"File not found: {file_path}\")\n",
    "            continue\n",
    "\n",
    "        # Read the context from file\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            context = f.read()\n",
    "\n",
    "        # Clean context\n",
    "        cleaned_context, removed_sentences = remove_similar_sentences(context, needle)\n",
    "\n",
    "        # Print removed sentences\n",
    "        print(f\"\\nðŸ§¹ File: {file_path}\")\n",
    "        for s in removed_sentences:\n",
    "            print(f\"Removed: {s}\")\n",
    "\n",
    "        # Overwrite the file with cleaned context\n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(cleaned_context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"FreshQADataset_with_misleading.xlsx\")  # Load your DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_files(df, 'context_relevant')\n",
    "clean_files(df, 'context_irrelevant')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
